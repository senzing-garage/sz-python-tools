#! /usr/bin/env python3

# TODO Shuffle file
# TODO Which cli args from G2Command
# TODO JB files glob request
# TODO CSVs
# TODO Check if input looks like JSON or not, don't try and load a CSV or other
# TODO Check for % of errors in first x records, quit if too high
# TODO Add env vars like here for all tools?
# TODO Warning message this is PoC tool like G2Loader?
# TODO Check memory and warning message - Jira to add psutil to V4
# TODO Runtime args at start or end and to errors/info file add complete input line + env vars if start with SENZING_

import argparse
import concurrent.futures
import itertools
import logging
import os
import pathlib
import signal
import sys
import textwrap
import time
from datetime import datetime
from threading import Event
from typing import Tuple

from _tool_helpers import get_max_futures_workers
from senzing import (
    SzBadInputError,
    SzConfigManager,
    SzDiagnostic,
    SzEngine,
    SzEngineFlags,
    SzError,
    SzProduct,
    SzRetryableError,
)

try:
    import orjson as json
except ModuleNotFoundError:
    import json

# TODO
__version__ = "1.3.0"  # See https://www.python.org/dev/peps/pep-0396/
__date__ = "2022-11-29"
__updated__ = "2023-12-19"

# TODO - Ant - Which version? Need name in demonstrable tools? Likely not
# LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s:  %(message)s"
LOG_FORMAT = "%(asctime)s - %(levelname)s:  %(message)s"
LONG_RECORD = 300
MODULE_NAME = pathlib.Path(sys.argv[0]).stem
PAYLOAD_RECORD = 0
PAYLOAD_START_TIME = 1
START_TS = str(datetime.now().strftime("%Y%m%d_%H%M%S"))
WORK_STATS_INTERVAL = 60

# TODO - Ant -
errors_file = f"{MODULE_NAME}_errors_{START_TS}.log"
with_info_file = f"{MODULE_NAME}_withInfo_{START_TS}.jsonl"
# If running in a container use /data/
if os.getenv("SENZING_DOCKER_LAUNCHED"):
    errors_file = f"/data/{errors_file}"
    with_info_file = f"/data/{with_info_file}"

# TODO - Ant - Move to helpers for other tools
try:
    logger = logging.getLogger(pathlib.Path(sys.argv[0]).stem)
    console_handle = logging.StreamHandler(stream=sys.stdout)
    console_handle.setLevel(logging.INFO)
    file_handle = logging.FileHandler(errors_file, "w")
    file_handle.setLevel(logging.ERROR)
    logger.setLevel(logging.INFO)
    logger.propagate = False
    console_handle.setFormatter(logging.Formatter(LOG_FORMAT))
    file_handle.setFormatter(logging.Formatter(LOG_FORMAT))
    logger.addHandler(console_handle)
    logger.addHandler(file_handle)
except IOError as err:
    print(err)
    sys.exit(1)

# TODO - Ant -
shutdown = Event()


# Custom actions for argparse. Enables checking if an arg "was specified" on the CLI to check if CLI args should take
# precedence over env vars and still can use the default setting for an arg if neither were specified.
class CustomArgActionStoreTrue(argparse.Action):
    """Set to true like using normal action=store_true and set _specified key for lookup"""

    def __call__(self, parser, namespace, values, option_string=None):
        setattr(namespace, self.dest, True)
        # TODO - Ant - f-string
        setattr(namespace, self.dest + "_specified", True)


class CustomArgAction(argparse.Action):
    """Set to value and set _specified key for lookup"""

    def __call__(self, parser, namespace, values, option_string=None):
        setattr(namespace, self.dest, values)
        # TODO - Ant - f-string
        setattr(namespace, self.dest + "_specified", True)


# TODO - Ant - Organise these
def parse_cli_args() -> argparse.Namespace:
    """# TODO"""
    # TODO - Ant -Change github link
    arg_parser = argparse.ArgumentParser(
        allow_abbrev=False,
        description="Utility to load Senzing JSON records and process redo records",
        epilog=textwrap.dedent(
            """\
            Arguments can be specified with either CLI arguments or environment variables, some arguments have
            default values.

            The order of precedence for selecting which value to use is:

            1) CLI Argument
            2) Environment variable
            3) Default value if available

            For additional help and information: https://github.com/Senzing/file-loader/blob/main/README.md

            """
        ),
        formatter_class=argparse.RawTextHelpFormatter,
    )

    arg_parser.add_argument(
        "-f",
        "--file",
        action=CustomArgAction,
        default=None,
        metavar="file",
        nargs="?",
        help=textwrap.dedent(
            """\
            Path and name of file to load.

            Default: None, skip loading but still process redo records
            Env Var: SENZING_INPUT_FILE

            """
        ),
    )
    arg_parser.add_argument(
        "-cj",
        "--configJson",
        action=CustomArgAction,
        default=None,
        metavar="config",
        nargs="?",
        type=str,
        help=textwrap.dedent(
            """\
            JSON string of the Senzing engine configuration.

            Default: None
            Env Var: SENZING_ENGINE_CONFIGURATION_JSON

            """
        ),
    )
    arg_parser.add_argument(
        "-w",
        "--withinfo",
        action=CustomArgActionStoreTrue,
        default=False,
        nargs=0,
        help=textwrap.dedent(
            """\
            Produce with info messages and write to a file.

            Default: False
            Env Var: SENZING_WITHINFO

            """
        ),
    )
    arg_parser.add_argument(
        "-t",
        "--debugTrace",
        action=CustomArgActionStoreTrue,
        default=False,
        nargs=0,
        help=textwrap.dedent(
            """\
            Output debug trace information.

            Default: False
            Env Var: SENZING_DEBUG

            """
        ),
    )
    # TODO - Ant - SENZING_THREADS_PER_PROCESS -> SENZING_NUMBER_OF_THREADS
    arg_parser.add_argument(
        "-nt",
        "--numThreads",
        action=CustomArgAction,
        default=0,
        metavar="num_threads",
        type=int,
        help=textwrap.dedent(
            """\
            Total number of worker threads performing load.

            Default: Calculated
            Env Var: SENZING_THREADS_PER_PROCESS

            """
        ),
    )
    arg_parser.add_argument(
        "-n",
        "--noRedo",
        action=CustomArgActionStoreTrue,
        default=False,
        nargs=0,
        help=argparse.SUPPRESS,
    )
    # TODO - Ant - Env vars? Display in startup
    # Args for Senzing team testing
    # Disable DS Perf
    arg_parser.add_argument(
        "-sdsp",
        "--skipDSPerf",
        action="store_true",
        default=False,
        help=argparse.SUPPRESS,
    )

    return arg_parser.parse_args()


# def get_logger(errors_file: str) -> logging.Logger:
#     """# TODO"""
#     try:
#         logger = logging.getLogger(pathlib.Path(sys.argv[0]).stem)
#         console_handle = logging.StreamHandler(stream=sys.stdout)
#         console_handle.setLevel(logging.INFO)
#         file_handle = logging.FileHandler(errors_file, "w")
#         file_handle.setLevel(logging.ERROR)
#         logger.setLevel(logging.INFO)
#         logger.propagate = False
#         log_format = "%(asctime)s - %(name)s - %(levelname)s:  %(message)s"
#         console_handle.setFormatter(logging.Formatter(log_format))
#         file_handle.setFormatter(logging.Formatter(log_format))
#         logger.addHandler(console_handle)
#         logger.addHandler(file_handle)
#     except IOError as err:
#         print(err)
#         sys.exit(1)

#     return logger


def check_ingest_file(in_file: str) -> bool:
    """# TODO"""
    json_errors = 0
    json_good = 0
    lines = []
    try:
        with open(in_file, "r", encoding="utf-8-sig") as file_:
            # TODO - Ant - Get first 100 lines and see if they pass here?
            # TODO - Ant - What if don't get line1 but no error?
            for _ in range(1, 101):
                read_line = file_.readline().strip()
                if read_line:
                    lines.append(read_line)
            # Test if JSON
            try:
                for line in lines:
                    _ = json.loads(line)
                    json_good += 1
            # TODO - Ant - orjson too
            except json.JSONDecodeError:
                json_errors += 1
                # try:
                #     csv_sample = file_.read(5000)
                #     csv_header = csv.Sniffer().has_header(csv_sample)
                #     csv_dialect = csv.Sniffer().sniff(csv_sample)
                #     # TODO - Ant -
                #     print(f"\n{csv_header = }")
                #     print(f"\n{csv_dialect.__dict__ = }")
                #     reader = csv.reader(file_, csv_dialect)
                #     for _ in range(500):
                #         row = next(reader)
                #         print(row)
                # except Exception as err:
                #     print(f"{err = }")

    except IOError as err:
        logger.error(f"Problem reading file {in_file}: {err}")
        return False

    # print(f"\n{json_good = }")
    # print(f"\n{json_errors = }")
    # print(f"\n{len(lines) = }")

    # TODO - Ant - No JSON lines at all, CSV?
    # TODO - Ant - Check the % of good v bad when hit a bad one?
    if (json_errors > json_good) and json_good == 0:
        logger.error(
            "File to ingest doesn't appear to be JSON. CSV files are not currently supported"
        )
        sys.exit(1)

    return True


def get_sz_engines(
    # logger: logging.Logger,
    engine_settings: str,
    debug_trace: bool,
) -> Tuple[SzEngine, SzDiagnostic, SzProduct, SzConfigManager]:
    """# TODO"""
    try:
        sz_engine = SzEngine("pySzEngine", engine_settings, verbose_logging=debug_trace)
        sz_diag = SzDiagnostic(
            "pySzDiagnostic", engine_settings, verbose_logging=debug_trace
        )
        sz_product = SzProduct(
            "pySzProduct", engine_settings, verbose_logging=debug_trace
        )
        sz_configmgr = SzConfigManager(
            "pySzConfigMgr", engine_settings, verbose_logging=debug_trace
        )
    except SzError as err:
        logger.error(err)
        sys.exit(1)

    return (sz_engine, sz_diag, sz_product, sz_configmgr)


# TODO - Ant -
def prime_sz_engine(sz_engine: SzEngine) -> None:
    """#TODO"""
    logger.info("Priming Senzing engine...")
    try:
        sz_engine.prime_engine()
    except SzError as err:
        logger.error(err)
        sys.exit(1)


def env_convert_boolean(env_var, cli_arg):
    """Convert boolean env var to True or False if set, otherwise use cli arg value"""
    evar = os.getenv(env_var)
    if evar:
        if isinstance(evar, str):
            if evar.lower() in ["true", "1", "t", "y", "yes"]:
                return True
            return False
        return evar

    return cli_arg


def startup_info(
    # logger: logging.Logger,
    engine: SzEngine,
    diag: SzDiagnostic,
    configmgr: SzConfigManager,
    product: SzProduct,
    skip_ds_perf: bool,
) -> None:
    """Fetch and display information at startup."""
    try:
        lic_info = json.loads(product.get_license())
        ver_info = json.loads(product.get_version())
        response = configmgr.get_configs()
        config_list = json.loads(response)
        active_cfg_id = engine.get_active_config_id()
        ds_info = json.loads(diag.get_datastore_info())
    except SzError as err:
        logger.error(f"Failed to get startup information: {err}")
        sys.exit(1)

    # Get details for the currently active config ID
    active_cfg_details = [
        details
        for details in config_list["CONFIGS"]
        if details["CONFIG_ID"] == active_cfg_id
    ]
    config_comments = active_cfg_details[0]["CONFIG_COMMENTS"]
    config_created = active_cfg_details[0]["SYS_CREATE_DT"]

    # Get data store info, build list of strings, could be a cluster
    ds_list = []
    for ds in ds_info["dataStores"]:
        ds_list.append(f"{ds['id']} - {ds['type']} - {ds['location']}")

    logger.info("Arguments & Environment")
    logger.info("-----------------------")
    logger.info("")
    logger.info(f'{" ".join(sys.argv)}')

    logger.info("")
    logger.info("Version & Configuration")
    logger.info("-----------------------")
    logger.info("")
    logger.info(
        "Senzing Version:           "
        f" {ver_info['VERSION'] + ' (' + ver_info['BUILD_DATE'] + ')'  if 'VERSION' in ver_info else ''}"
    )
    logger.info(f"Instance Config ID:         {active_cfg_id}")
    logger.info(f"Instance Config Comments:   {config_comments}")
    logger.info(f"Instance Config Created:    {config_created}")
    logger.info(f"Datastore(s):               {ds_list.pop(0)}")
    for ds in ds_list:
        logger.info(f"{' ' * 28}{ds}")
    logger.info("")
    logger.info("License")
    logger.info("-------")
    logger.info("")
    logger.info(f'Customer:    {lic_info["customer"]}')
    logger.info(f'Type:        {lic_info["licenseType"]}')
    logger.info(f'Records:     {lic_info["recordLimit"]}')
    logger.info(f'Expiration:  {lic_info["expireDate"]}')
    logger.info(f'Contract:    {lic_info["contract"]}')
    logger.info("")

    # Skip perf check if specified on CLI args or container env var
    if not skip_ds_perf:
        max_time_per_insert = 0.5
        # TODO - Ant - Check is in V4 docs
        db_tune_article = "https://senzing.zendesk.com/hc/en-us/articles/360016288254-Tuning-Your-Database"

        logger.info("")
        logger.info("Datastore Performance")
        logger.info("--------------------")
        logger.info("")

        ds_perf = json.loads(diag.check_datastore_performance(3))
        num_recs_inserted = ds_perf.get("numRecordsInserted", None)
        if num_recs_inserted:
            insert_time = ds_perf["insertTime"]
            time_per_insert = (
                (1.0 * insert_time / num_recs_inserted)
                if num_recs_inserted > 0
                else 999
            )
            logger.info(f"Records inserted:    {num_recs_inserted:,}")
            logger.info(f"Period for inserts:  {insert_time} ms")
            logger.info(f"Average per insert:  {time_per_insert:.1f} ms")
            logger.info("")
        else:
            logger.error("Datastore performance test failed!")

        if time_per_insert > max_time_per_insert:
            logger.warning(
                f"Datastore performance of {time_per_insert:.1f} ms per insert is slower than the recommended minimum of {max_time_per_insert:.1f} ms per insert"
            )
            logger.warning(f"For database tuning refer to: {db_tune_article}")
            logger.warning("")
            logger.warning("Pausing for warning message...")
            logger.warning("")
            time.sleep(5)


def add_record(engine, rec_to_add, with_info):
    """Add a single record, returning with info details if --info or SENZING_WITHINFO was specified"""
    record_dict = json.loads(rec_to_add)
    data_source = record_dict.get("DATA_SOURCE", None)
    record_id = record_dict.get("RECORD_ID", None)

    if with_info:
        response = engine.add_record(
            data_source, record_id, rec_to_add, SzEngineFlags.SZ_WITH_INFO
        )
        return response

    engine.add_record(data_source, record_id, rec_to_add)
    return None


def get_redo_record(engine):
    """Get a redo record for processing"""
    try:
        redo_record = engine.get_redo_record()
    except SzError as err:
        logger.critical(f"Exception: {err} - Operation: getRedoRecord")
        # TODO - Ant -
        # global do_shutdown
        # do_shutdown = True
        shutdown.set()
        # TODO For typing this would be ""
        return None

    return redo_record


def prime_redo_records(engine, quantity):
    """Get a specified number of redo records for priming processing"""
    redo_records = []
    for _ in range(quantity):
        single_redo_rec = get_redo_record(engine)
        if single_redo_rec:
            redo_records.append(single_redo_rec)
    return redo_records


def process_redo_record(engine, record, with_info):
    """Process a single redo record, returning with info details if --info or SENZING_WITHINFO was specified"""
    if with_info:
        response = engine.process_redo_record(record, SzEngineFlags.SZ_WITH_INFO)
        return response

    engine.process_redo_record(record)
    return None


def record_stats(success_recs, error_recs, prev_time, operation):
    """Log details on records for add/redo"""
    logger.info(
        f"Processed {success_recs:,} {operation},"
        f" {int(1000 / (time.time() - prev_time)):,} records per second,"
        f" {error_recs} errors"
    )
    return time.time()


def workload_stats(engine):
    """Log engine workload stats"""
    try:
        stats = engine.get_stats()
        logger.info("")
        logger.info(stats)
        logger.info("")
    except SzError as err:
        logger.critical(f"Exception: {err} - Operation: get_stats")
        # TODO - Ant -
        # global do_shutdown
        # do_shutdown = True
        shutdown.set()


def long_running_check(futures, time_now, num_workers):
    """Check for long-running records"""
    num_stuck = 0
    for fut, payload in futures.items():
        if not fut.done():
            duration = time_now - payload[PAYLOAD_START_TIME]
            if duration > LONG_RECORD:
                num_stuck += 1
                stuck_record = json.loads(payload[PAYLOAD_RECORD])
                logger.warning(
                    f"Long running record ({duration / 60:.3g}):"
                    f" {stuck_record['DATA_SOURCE']} - {stuck_record['RECORD_ID']}"
                )

    if num_stuck >= num_workers:
        logger.warning(
            f"All {num_workers} threads are stuck processing long running records"
        )


def signal_int(signum, frame):
    """Interrupt to allow running threads to finish"""
    logger.warning(
        "Please wait for running tasks to complete, this could take many minutes...\n"
    )
    # TODO - Ant -
    # global do_shutdown
    # do_shutdown = True
    shutdown.set()


def load_and_redo(
    cli_args,
    sz_engine,
    ingest_file,
    num_workers,
    no_redo,
    with_info,
):
    """Load records and process redo records after loading is complete"""

    def add_new_future():
        """Add a new feature as needed"""
        if mode.__name__ == "add_record":
            record = in_file.readline()
        else:
            record = get_redo_record(sz_engine)

        if record:
            futures[
                executor.submit(
                    mode,
                    sz_engine,
                    record,
                    with_info,
                )
            ] = (record, time.time())
            return False
        else:
            return True

    # TODO - Ant -
    # global do_shutdown
    error_recs = load_errors = load_success = load_time = redo_errors = redo_success = (
        redo_time
    ) = 0

    mode_text = {
        "add_record": {
            "start_msg": "Starting to load with",
            "except_msg": "add_record",
            "results_header": "Loading",
            "results_rec_type": "load",
            "stats_msg": "adds",
        },
        "process_redo_record": {
            "start_msg": "Starting to process redo records with",
            "except_msg": "process_redo_record",
            "results_header": "Redo",
            "results_rec_type": "redo",
            "stats_msg": "redos",
        },
    }

    # Test the max number of workers ThreadPoolExecutor allocates to use in sizing actual workers to request
    max_workers = num_workers if num_workers else get_max_futures_workers()

    if ingest_file:
        in_file = open(ingest_file, "r", encoding="utf-8-sig")

    modes = [add_record] if no_redo else [add_record, process_redo_record]
    main_start_time = time.time()

    with open(with_info_file, "w", encoding="utf-8") as out_file:
        for mode in modes:
            # If loading is stopped or fails don't process redo
            # TODO - Ant -
            # if do_shutdown:
            if shutdown.is_set():
                break

            add_future = True
            end_of_recs = False
            error_recs = 0
            load_time = 0
            success_recs = 0

            # If the file was empty or no file was specified skip loading
            if mode.__name__ == "add_record" and not ingest_file:
                logger.info("")
                logger.info(
                    "No input file. Skipping loading, checking for redo records..."
                )
                # load_time = 0
                continue

            start_time = long_check_time = work_stats_time = prev_time = time.time()

            with concurrent.futures.ThreadPoolExecutor(max_workers) as executor:
                workers_max = executor._max_workers  # pylint: disable=protected-access
                if mode.__name__ == "add_record":
                    futures = {
                        executor.submit(mode, sz_engine, record, with_info): (
                            record,
                            time.time(),
                        )
                        for record in itertools.islice(in_file, workers_max)
                    }
                else:
                    futures = {
                        executor.submit(mode, sz_engine, record, with_info): (
                            record,
                            time.time(),
                        )
                        for record in prime_redo_records(sz_engine, workers_max)
                    }
                logger.info("")
                logger.info(
                    f"{mode_text[mode.__name__]['start_msg']} {workers_max} threads..."
                )
                logger.info("")

                while futures:
                    done, _ = concurrent.futures.wait(
                        futures, return_when=concurrent.futures.FIRST_COMPLETED
                    )
                    for f in done:
                        try:
                            result = f.result()
                        except (
                            SzBadInputError,
                            SzRetryableError,
                            json.JSONDecodeError,
                        ) as err:
                            logger.error(
                                f"Exception: {err} - Operation:"
                                f" {mode_text[mode.__name__]['except_msg']} -"
                                f" Record: {futures[f][PAYLOAD_RECORD]}"
                            )
                            error_recs += 1
                        except SzError as err:
                            logger.critical(
                                f"Exception: {err} - Operation:"
                                f" {mode_text[mode.__name__]['except_msg']} -"
                                f" Record: {futures[f][PAYLOAD_RECORD]}"
                            )
                            error_recs += 1
                            # TODO - Ant -
                            # do_shutdown = True
                            shutdown.set()
                        else:
                            # TODO - Ant -
                            # if add_future and not do_shutdown:
                            if add_future and not shutdown.is_set():
                                end_of_recs = add_new_future()

                            if result:
                                out_file.write(f"{result}\n")

                            success_recs += 1
                            if success_recs % 1000 == 0:
                                prev_time = record_stats(
                                    success_recs,
                                    error_recs,
                                    prev_time,
                                    mode_text[mode.__name__]["stats_msg"],
                                )
                        finally:
                            del futures[f]

                    # # TODO Keep?
                    # Early errors check to catch mis-mapping, missing dsrc_code, etc
                    if error_recs == workers_max:
                        logger.info("")
                        # TODO Does this show the errors file?
                        logger.error(
                            f"All initial {workers_max:,} records failed! Stopping processing, please review the errors file."
                        )
                        logger.info("")
                        # TODO - Ant -
                        # do_shutdown = True
                        shutdown.set()
                    # if success_recs <= 100 and error_recs > 50:
                    #     logger.critical(
                    #         f"Shutting down - {success_recs = } - {error_recs = }"
                    #     )
                    #     print(f"{len(futures) = }")
                    #     do_shutdown = True

                    # TODO - Ant -
                    # if (do_shutdown or end_of_recs) and len(futures) == 0:
                    if (shutdown.is_set() or end_of_recs) and len(futures) == 0:
                        break

                    time_now = time.time()
                    if time_now > work_stats_time + WORK_STATS_INTERVAL:
                        work_stats_time = time_now
                        workload_stats(sz_engine)

                    if time_now > long_check_time + LONG_RECORD:
                        long_check_time = time_now
                        long_running_check(futures, time_now, executor._max_workers)

            # TODO - Ant -
            # if not do_shutdown:
            if not shutdown.is_set():
                workload_stats(sz_engine)

            # TODO - Ant -
            # if do_shutdown:
            if shutdown.is_set():
                logger.warning("Processing was interrupted, shutting down...")
                logger.info("")

            # TODO Add same for redo so don't get 100 error on redo when first 100 load fail
            # Store loading stats for overall results stats
            if mode.__name__ == "add_record":
                load_time = round((time.time() - main_start_time) / 60, 1)
                load_errors = error_recs
                load_success = success_recs
                # TODO - Ant -
                # if not do_shutdown:
                if not shutdown.is_set():
                    logger.info(
                        f"Successfully loaded {load_success:,} records in"
                        f" {load_time} mins with"
                        f" {load_errors:,} error(s)"
                    )
            else:
                redo_time = 0 if no_redo else round((time.time() - start_time) / 60, 1)
                redo_errors = error_recs
                redo_success = success_recs

        if no_redo:
            logger.info("")
        logger.info("Results")
        logger.info("-------")
        logger.info("")
        logger.info(
            # TODO Make files Path objects earlier
            "Source file:                "
            f"{pathlib.Path(ingest_file).resolve() if ingest_file else 'No input file specified'}"
        )
        logger.info(
            "With info file:             "
            f"{pathlib.Path(file_output).resolve() if with_info else 'With info responses not requested'}"
        )
        logger.info(
            "Errors file:                "
            f"{pathlib.Path(errors_file).resolve() if (load_errors + error_recs) > 0 else 'No errors'}"
        )
        logger.info("")
        # TODO Language?
        logger.info(f"Successful loaded records:  {load_success:,}")
        logger.info(f"Error loaded records:       {load_errors:,}")
        logger.info(f"Loading elapsed time (s):   {load_time}")
        logger.info("")
        logger.info(
            # f"Successful redo records:    {f'{redo_success:,}' if not no_redo else no_redo_msg}"
            f"Successful redo records:    {f'{redo_success:,}' if not no_redo else 'Redo disabled'}"
        )
        logger.info(
            f"Error redo records:         {f'{redo_errors:,}' if not no_redo else ''}"
        )
        logger.info(f"Redo elapsed time (s):      {redo_time if not no_redo else ''}")
        logger.info("")
        logger.info(f"Total elapsed time (s):     {load_time + redo_time}")

        if not cli_args.withinfo and not os.getenv("SENZING_WITHINFO"):
            pathlib.Path(with_info_file).unlink(missing_ok=True)

        if not load_errors and not error_recs:
            pathlib.Path(errors_file).unlink(missing_ok=True)

        if ingest_file:
            in_file.close()


def main() -> None:
    """# TODO"""

    signal.signal(signal.SIGINT, signal_int)

    # MODULE_NAME = pathlib.Path(sys.argv[0]).stem

    # do_shutdown = False

    cli_args = parse_cli_args()

    # If a CLI arg was specified use it, else try the env var, if no env var use the default for the CLI arg
    # Sets the priority to: 1) CLI arg, 2) Env Var 3) Default value
    ingest_file = (
        cli_args.file
        if cli_args.__dict__.get("file_specified")
        else os.getenv("SENZING_INPUT_FILE")
    )

    engine_settings = (
        cli_args.configJson
        if cli_args.__dict__.get("configJson_specified")
        else os.getenv("SENZING_ENGINE_CONFIGURATION_JSON", cli_args.configJson)
    )

    with_info = (
        cli_args.withinfo
        if cli_args.__dict__.get("info_specified")
        else env_convert_boolean("SENZING_WITHINFO", cli_args.withinfo)
    )

    debug_trace = (
        cli_args.debugTrace
        if cli_args.__dict__.get("debugTrace_specified")
        else env_convert_boolean("SENZING_DEBUG", cli_args.debugTrace)
    )

    num_threads = (
        cli_args.numThreads
        if cli_args.__dict__.get("numThreads_specified")
        else int(os.getenv("SENZING_THREADS_PER_PROCESS", cli_args.numThreads))
    )

    no_redo = (
        cli_args.noRedo
        if cli_args.__dict__.get("noRedo_specified")
        else env_convert_boolean("SENZING_NO_REDO", cli_args.noRedo)
    )

    skip_ds_perf = (
        cli_args.skipDSPerf
        if cli_args.__dict__.get("skipDSPerf_specified")
        else env_convert_boolean(
            "SENZING_SKIP_DATABASE_PERFORMANCE_TEST", cli_args.skipDSPerf
        )
    )

    # errors_file = (
    #     f'{MODULE_NAME}_errors_{str(datetime.now().strftime("%Y%m%d_%H%M%S"))}.log'
    # )

    # with_info_file = (
    #     f'{MODULE_NAME}_withInfo_{str(datetime.now().strftime("%Y%m%d_%H%M%S"))}.jsonl'
    # )

    # # If running in a container use /data/
    # if os.getenv("SENZING_DOCKER_LAUNCHED"):
    #     errors_file = f"/data/{errors_file}"
    #     with_info_file = f"/data/{with_info_file}"

    # logger = get_logger(errors_file)

    # TODO - Ant - Correct article link for V4?
    if not engine_settings:
        logger.warning(
            "SENZING_ENGINE_CONFIGURATION_JSON environment variable or --configJson CLI"
            " argument must be set with the engine configuration JSON"
        )
        logger.warning(
            "https://senzing.zendesk.com/hc/en-us/articles/360038774134-G2Module-Configuration-and-the-Senzing-API"
        )
        sys.exit(1)

    if not ingest_file and no_redo:
        logger.error("No input file and redo processing disabled, nothing to do!")
        sys.exit(1)

    check_ingest_file(ingest_file)

    sz_engine, sz_diag, sz_product, sz_configmgr = get_sz_engines(
        engine_settings, debug_trace
    )

    startup_info(sz_engine, sz_diag, sz_configmgr, sz_product, skip_ds_perf)
    prime_sz_engine(sz_engine)
    load_and_redo(
        cli_args,
        sz_engine,
        ingest_file,
        num_threads,
        no_redo,
        with_info,
    )


if __name__ == "__main__":
    main()

# TODO - Ant -
# if __name__ == "__main__":
#     sys.exit(main(sys.argv))
