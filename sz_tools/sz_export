#! /usr/bin/env python3


import argparse
import contextlib
import csv
import gzip
import pathlib
import sys
import textwrap
import time
from datetime import datetime

from _tool_helpers import (
    combine_engine_flags,
    get_engine_config,
    print_error,
    print_warning,
)
from senzing import SzEngineFlags, SzError
from senzing_core import SzAbstractFactoryCore

MODULE_NAME = pathlib.Path(__file__).stem
VALID_FLAGS = [
    "SZ_ENTITY_BRIEF_DEFAULT_FLAGS",
    "SZ_ENTITY_CORE_FLAGS",
    "SZ_ENTITY_DEFAULT_FLAGS",
    "SZ_ENTITY_INCLUDE_ALL_FEATURES",
    "SZ_ENTITY_INCLUDE_ALL_RELATIONS",
    "SZ_ENTITY_INCLUDE_DISCLOSED_RELATIONS",
    "SZ_ENTITY_INCLUDE_ENTITY_NAME",
    "SZ_ENTITY_INCLUDE_INTERNAL_FEATURES",
    "SZ_ENTITY_INCLUDE_FEATURE_STATS",
    "SZ_ENTITY_INCLUDE_NAME_ONLY_RELATIONS",
    "SZ_ENTITY_INCLUDE_POSSIBLY_RELATED_RELATIONS",
    "SZ_ENTITY_INCLUDE_POSSIBLY_SAME_RELATIONS",
    "SZ_ENTITY_INCLUDE_RECORD_DATA",
    "SZ_ENTITY_INCLUDE_RECORD_FEATURE_DETAILS",
    "SZ_ENTITY_INCLUDE_RECORD_FEATURES",
    "SZ_ENTITY_INCLUDE_RECORD_FEATURE_STATS",
    "SZ_ENTITY_INCLUDE_RECORD_JSON_DATA",
    "SZ_ENTITY_INCLUDE_RECORD_MATCHING_INFO",
    "SZ_ENTITY_INCLUDE_RECORD_SUMMARY",
    "SZ_ENTITY_INCLUDE_RECORD_TYPES",
    "SZ_ENTITY_INCLUDE_RECORD_UNMAPPED_DATA",
    "SZ_ENTITY_INCLUDE_RELATED_ENTITY_NAME",
    "SZ_ENTITY_INCLUDE_RELATED_MATCHING_INFO",
    "SZ_ENTITY_INCLUDE_RELATED_RECORD_DATA",
    "SZ_ENTITY_INCLUDE_RELATED_RECORD_SUMMARY",
    "SZ_ENTITY_INCLUDE_RELATED_RECORD_TYPES",
    "SZ_ENTITY_INCLUDE_REPRESENTATIVE_FEATURES",
    "SZ_ENTITY_INCLUDE_RECORD_UNMAPPED_DATA",
    "SZ_EXPORT_DEFAULT_FLAGS",
    "SZ_EXPORT_INCLUDE_ALL_ENTITIES",
    "SZ_EXPORT_INCLUDE_ALL_HAVING_RELATIONSHIPS",
    "SZ_EXPORT_INCLUDE_NAME_ONLY",
    "SZ_EXPORT_INCLUDE_DISCLOSED",
    "SZ_EXPORT_INCLUDE_MULTI_RECORD_ENTITIES",
    "SZ_EXPORT_INCLUDE_POSSIBLY_RELATED",
    "SZ_EXPORT_INCLUDE_POSSIBLY_SAME",
    "SZ_EXPORT_INCLUDE_SINGLE_RECORD_ENTITIES",
    "SZ_INCLUDE_MATCH_KEY_DETAILS",
    "SZ_RECORD_DEFAULT_FLAGS",
]


def csv_fetch_next(handle, csv_header=None):
    """Fetch next for CSV output"""

    try:
        export_record = sz_engine.fetch_next(handle)
    except SzError as err:
        print_error(err, exit_=True)

    # If no csv_header is sent we fetched the header row initially
    if not csv_header:
        return export_record.strip().split(",")

    # Check data doesn't exceed the csv field limit
    if len(export_record) > csv.field_size_limit():
        csv.field_size_limit(int(len(export_record) * 1.5))
        print(
            f"    Increased CSV field limit size to: {csv.field_size_limit()}",
        )
    export_record_dict = next(csv.DictReader([export_record], fieldnames=csv_header)) if export_record else None

    return export_record, export_record_dict


def json_fetch_next(handle):
    """Fetch next for JSON output"""

    try:
        export_record = sz_engine.fetch_next(handle)
    except SzError as err:
        print_error(err, exit_=True)

    return export_record


def do_stats_output(total_entity_count, start_time, batch_row_count):
    """Print stats if output frequency interval and not disabled with -1. Reset batch row count if triggered"""

    if args.outputFrequency != -1 and total_entity_count % args.outputFrequency == 0:
        time_now = datetime.now().strftime("%I:%M:%S %p").lower()
        rows_per_sec = int(
            float(batch_row_count) / (float(time.time() - start_time if time.time() - start_time != 0 else 1))
        )
        ents_per_sec = int(
            float(args.outputFrequency) / (float(time.time() - start_time if time.time() - start_time != 0 else 1))
        )
        print(
            f"{total_entity_count:,} entities processed at {time_now} ({ents_per_sec:,} per second), {rows_per_sec:,} rows per second",
        )

        start_time = time.time()
        batch_row_count = 0

    return start_time, batch_row_count


def csv_export():
    """Export data in CSV format"""

    bad_count_inner = 0
    bad_count_outer = 0
    batch_row_count = 0
    entity_count = 0
    fetched_rec_count = 0
    total_entity_count = 0
    total_row_count = 0

    # First row is header for CSV
    csv_header = csv_fetch_next(export_handle)

    # Create writer object and write the header row
    try:
        writer = csv.DictWriter(
            output_file,
            fieldnames=csv_header,
            dialect=csv.excel,
            quoting=csv.QUOTE_ALL,
        )
        writer.writeheader()
    except csv.Error as err:
        print_error(f"Could not create CSV writer for output or write CSF header: {err}", exit_=True)

    start_time = time.time()

    # Read rows from the export handle
    export_record, export_record_dict = csv_fetch_next(export_handle, csv_header)

    while export_record:

        row_list = []
        fetched_rec_count += 1
        batch_row_count += 1

        # Bypass bad rows
        if "RESOLVED_ENTITY_ID" not in export_record_dict:
            print_error(f"RESOLVED_ENTITY_ID is missing at line: {fetched_rec_count}: - {export_record.strip()}")
            (export_record, export_record_dict) = csv_fetch_next(export_handle, csv_header)
            bad_count_outer += 1
            fetched_rec_count += 1
            continue

        resolved_entity_id = export_record_dict["RESOLVED_ENTITY_ID"]

        # Keep fetching all export rows for the current RES_ENT
        while export_record_dict and export_record_dict["RESOLVED_ENTITY_ID"] == resolved_entity_id:

            # Bypass bad rows
            if "RECORD_ID" not in export_record_dict:
                print_error(f"RECORD_ID is missing at line: {fetched_rec_count} - {export_record.strip()}")
                (export_record, export_record_dict) = csv_fetch_next(export_handle, csv_header)
                bad_count_inner += 1
                fetched_rec_count += 1
                continue

            # Strip leading symbols on match_key
            if export_record_dict["MATCH_KEY"] and export_record_dict["MATCH_KEY"][0:1] in ("+", "-"):
                export_record_dict["MATCH_KEY"] = export_record_dict["MATCH_KEY"][1:]

            # For CSV output with extended don't include JSON_DATA (unless -xcr is used)
            if args.extended and not args.extendCSVRelates and export_record_dict["RELATED_ENTITY_ID"] != "0":
                export_record_dict.pop("JSON_DATA", None)

            row_list.append(export_record_dict)
            (export_record, export_record_dict) = csv_fetch_next(export_handle, csv_header)
            fetched_rec_count += 1
            batch_row_count += 1

        entity_count += 1
        total_entity_count += 1

        # Write the rows for the entity
        try:
            writer.writerows(row_list)
        except Exception as err:
            print_error(f"Writing to CSV file: {err}")
            return total_row_count, (bad_count_outer + bad_count_inner), 1
        total_row_count += len(row_list)

        (start_time, batch_row_count) = do_stats_output(total_entity_count, start_time, batch_row_count)

    return total_row_count, (bad_count_outer + bad_count_inner), 0


def json_export():
    """Export data in JSON format"""

    row_count = batch_row_count = 0
    start_time = time.time()

    export_record = json_fetch_next(export_handle)

    while export_record:

        row_count += 1
        batch_row_count += 1

        try:
            output_file.write(export_record)
        except IOError as err:
            print_error(f"Writing to JSON file: {err}")
            return row_count, 0, 1

        start_time, batch_row_count = do_stats_output(row_count, start_time, batch_row_count)

        export_record = json_fetch_next(export_handle)

    return row_count, 0, 0


@contextlib.contextmanager
def open_file(file_name):
    """Use with open context to open either a file or compressed file"""
    handle = (
        gzip.open(file_name, "wt", compresslevel=args.compressFile)
        if args.compressFile
        else open(file_name, "w", encoding="utf-8")
    )

    try:
        yield handle
    finally:
        handle.close()


if __name__ == "__main__":
    cli_args = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter, allow_abbrev=False)
    cli_args.add_argument(
        "-o",
        "--output_file",
        required=True,
        nargs="?",
        help=textwrap.dedent(
            """\

            Path and file name to send output to.

            """
        ),
    )

    cli_args.add_argument(
        "-c",
        "--iniFile",
        dest="ini_file_name",
        default=None,
        nargs=1,
        help=textwrap.dedent(
            """\

            Path and file name of optional sz_engine_config.ini to use.

            """
        ),
    )

    cli_args.add_argument(
        "-f",
        "--flags",
        default=["SZ_ENTITY_BRIEF_DEFAULT_FLAGS", "SZ_EXPORT_INCLUDE_ALL_ENTITIES"],
        nargs="+",
        help=textwrap.dedent(
            """\

            Space separated list of export flags to apply to the export.

            Default: %(default)s

            """
        ),
    )

    cli_args.add_argument(
        "-F",
        "--outputFormat",
        default="CSV",
        type=str.upper,
        choices=("CSV", "JSON"),
        help=textwrap.dedent(
            """\

            Data format to export to, JSON or CSV.

            Default: %(default)s

            """
        ),
    )

    cli_args.add_argument(
        "-x",
        "--extended",
        default=False,
        action="store_true",
        help=textwrap.dedent(
            """\

            Return extended details, adds RESOLVED_ENTITY_NAME & JSON_DATA.

            Adding JSON_DATA significantly increases the size of the output and execution time.

            When used with CSV output, JSON_DATA isn\'t included for the related entities
            (RELATED_ENTITY_ID) for each resolved entity (RESOLVED_ENTITY_ID). This reduces
            the size of a CSV export by preventing repeating data for related entities. JSON_DATA
            for the related entities is still included in the CSV export and is located in the
            export record where the RELATED_ENTITY_ID = RESOLVED_ENTITY_ID.

            WARNING: This is not recommended! To include the JSON_DATA for every CSV record see the
            --extendCSVRelates (-xcr) argument.

            """
        ),
    )

    cli_args.add_argument(
        "-of",
        "--outputFrequency",
        default=1000,
        type=int,
        help=textwrap.dedent(
            """\

            Frequency of export output statistics.

            Default: %(default)s

            """
        ),
    )

    cli_args.add_argument(
        "-cf",
        "--compressFile",
        default=None,
        const=6,
        nargs="?",
        type=int,
        choices=range(1, 10),
        help=textwrap.dedent(
            f"""\

            Compress output file with gzip. Compression level can be optionally specified.

            If output file is specified as - (for stdout), use shell redirection instead to compress:
            {MODULE_NAME} -o - | gzip -v > export.csv.gz

            Default: %(const)s

            """
        ),
    )

    cli_args.add_argument(
        "-xcr",
        "--extendCSVRelates",
        default=False,
        action="store_true",
        help=textwrap.dedent(
            """\

            WARNING: Use of this argument is not recommended!

            Used in addition to --extend (-x), it will include JSON_DATA in CSV output for related entities.

            Only valid for CSV output format.

            """
        ),
    )

    args = cli_args.parse_args()

    if args.extendCSVRelates and not args.extended:
        print(
            textwrap.dedent(
                f"""
                ERROR: Argument --extendCSVRelates (-xcr) is used to complement --extended (-x) and not used alone.')

                        Review the help with {MODULE_NAME} --help
            """
            ),
        )
        sys.exit(0)

    if args.extendCSVRelates:
        print_warning(
            textwrap.dedent(
                f"""

                ***************************************************** WARNING ****************************************************

                Using the --extendCSVRelates (-xcr) argument with CSV output format will result in excessive and repeated data for
                related entities. Very rarely, if ever, is this option required!

                Hit CTRL-C to exit or wait 10 seconds to continue.

                Review the help with {MODULE_NAME} --help

                ******************************************************************************************************************
                """
            ),
        )

        time.sleep(10)

    print_warning(
        textwrap.dedent(
            f"""

                ******************************************************** WARNING *******************************************************

                {MODULE_NAME} isn't intended for exporting large numbers of entities and associated data source record information.
                Beyond 100M+ data source records isn't suggested. For exporting overview entity and relationship data for
                analytical purposes outside of Senzing please review the following article:

                https://senzing.zendesk.com/hc/en-us/articles/360010716274--Advanced-Replicating-the-Senzing-results-to-a-Data-Warehouse

                ************************************************************************************************************************
                """
        ),
    )

    time.sleep(5)

    # Some CSV exports can be large especially with extended data. Is checked and increased in csv_fetch_next()
    csv.field_size_limit(300000)

    # Fields to use with CSV output, list of fields to request data
    # For CSV these are unioned with the data returned by the flags to give final output
    csvFields = [
        "RESOLVED_ENTITY_ID",
        "RELATED_ENTITY_ID",
        "MATCH_LEVEL",
        "MATCH_KEY",
        "DATA_SOURCE",
        "RECORD_ID",
    ]
    if args.extended:
        csvFields.insert(2, "RESOLVED_ENTITY_NAME")
        csvFields.insert(6, "JSON_DATA")

    # Check can locate an engine configuration
    engine_config = get_engine_config(args.ini_file_name)

    try:
        sz_factory = SzAbstractFactoryCore(MODULE_NAME, engine_config)
        sz_engine = sz_factory.create_engine()
    except SzError as err:
        print_error(err, exit_=True)
        sys.exit(1)

    # Convert strings to upper and if integers supplied convert from string to int
    flags = [int(flag) if flag.isdigit() else flag.upper() for flag in args.flags]

    # Get only the string flags to check against accepted flags
    string_flags = [flag for flag in flags if isinstance(flag, str)]
    invalid_string_flags = set(string_flags).difference(VALID_FLAGS)
    if invalid_string_flags:
        print_warning("Ignoring invalid flag(s): ", end_str=" ")
        print(", ".join(invalid_string_flags))

    valid_flags = [flag for flag in flags if flag not in invalid_string_flags]
    final_flags = combine_engine_flags(valid_flags)

    # Initialize the export
    export_output = args.output_file
    if args.compressFile:
        export_output = f"{args.output_file}.gz"

    # Open file for export output
    with open_file(export_output) as output_file:
        # Create CSV or JSON export handle to fetch from
        try:
            if args.outputFormat == "CSV":
                CSV_FIELDS_STR = ", ".join(csvFields)
                export_handle = sz_engine.export_csv_entity_report(CSV_FIELDS_STR, final_flags)
            else:
                export_handle = sz_engine.export_json_entity_report(final_flags)
        except SzError as err:
            print_error(f"Could not initialize export: {err}", exit_=True)

        export_start = time.time()

        row_count, bad_rec_count, exit_code = csv_export() if args.outputFormat == "CSV" else json_export()

        if exit_code:
            print("ERROR: Export did not complete successfully!")
            sys.exit(1)

    export_finish = time.time()

    # Display information for reference
    print(
        textwrap.dedent(
            f"""
            Export output file:          {export_output}
            Export output format:        {args.outputFormat}
            Exported rows:               {row_count:,}
            Bad rows skipped:            {bad_rec_count}
            Execution time:              {round((export_finish - export_start) / 60, 1)} mins
            """
        ),
    )
