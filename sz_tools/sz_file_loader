#! /usr/bin/env python3

import argparse
import atexit
import concurrent.futures
import itertools
import logging
import os
import signal
import subprocess
import sys
import textwrap
import time
from collections import namedtuple
from contextlib import suppress
from datetime import datetime
from pathlib import Path
from threading import Event
from types import FrameType
from typing import Any, Iterable, List, TextIO, Tuple, TypeVar, Union

from _tool_helpers import (
    check_file_exists,
    check_path_writeable,
    get_char_with_timeout,
    get_engine_config,
    get_max_futures_workers,
    human_readable_bytes,
    in_docker,
)
from senzing import (
    SzConfigManager,
    SzDiagnostic,
    SzEngine,
    SzEngineFlags,
    SzError,
    SzProduct,
    SzRetryableError,
    SzUnrecoverableError,
)
from senzing_core import SzAbstractFactoryCore

try:
    import orjson

    # def _json_dumps(object_: Any) -> str:
    # return orjson.dumps(object_).decode("utf-8")

    def _json_loads(object_: Any) -> Any:
        return orjson.loads(object_)

    JSONDecodeError = orjson.JSONDecodeError
    # JSONEncodeError = orjson.JSONEncodeError

except ImportError:
    import json

    # def _json_dumps(object_: Any) -> str:
    #     return json.dumps(object_, ensure_ascii=False)

    def _json_loads(object_: Any) -> Any:
        return json.loads(object_)

    JSONDecodeError = json.JSONDecodeError


LOG_FORMAT = "%(asctime)s - %(levelname)s: %(message)s"
LONG_RECORD = 300
MODE_TEXT = {
    "add_record": {
        "start_msg": "Starting to load with",
        "except_msg": "add_record",
        "results_header": "Loading",
        "results_rec_type": "load",
        "stats_msg": "adds",
    },
    "process_redo_record": {
        "start_msg": "Starting to process redo records with",
        "except_msg": "process_redo_record",
        "results_header": "Redo",
        "results_rec_type": "redo",
        "stats_msg": "redos",
    },
}
MODULE_NAME = Path(__file__).stem
START_TS = str(datetime.now().strftime("%Y%m%d_%H%M%S"))
SHUFF_NO_DEL_TAG = "_sz_shuff_no_del_"
SHUFF_TAG = "_sz_shuff_"
SHUFF_GLOB_TAG = "_sz_shuff*"
SHUFF_TIMEOUT = 30

T = TypeVar("T")


class ConsoleErrorFormatter(logging.Formatter):
    """Custom logging formatting for console output"""

    def format_exception(self, exc_info):
        """Format errors and warnings to the console prefixed with ERROR|WARNING:"""
        return repr(super().formatException(exc_info))

    def format(self, record):
        f_record = super().format(record)
        if record.levelname in ("ERROR", "WARNING"):
            return f"{record.levelname}: {f_record}"

        return f_record


try:
    logger = logging.getLogger(Path(sys.argv[0]).stem)
    logger.setLevel(logging.INFO)
    logger.propagate = False
    console_handle = logging.StreamHandler(stream=sys.stdout)
    console_handle.setLevel(logging.INFO)
    console_handle.setFormatter(ConsoleErrorFormatter(""))
    logger.addHandler(console_handle)
except OSError as err_outer:
    print(f"\nERROR: Couldn't create logger: {err_outer}")
    sys.exit(1)

shutdown = Event()


def files_clean_up(errors_file: Path, with_info_file: Path) -> None:
    """TODO"""
    if check_file_exists(errors_file) and errors_file.stat().st_size == 0:
        errors_file.unlink()

    if check_file_exists(with_info_file) and with_info_file.stat().st_size == 0:
        with_info_file.unlink()


def parse_cli_args() -> argparse.Namespace:
    """TODO"""
    arg_parser = argparse.ArgumentParser(
        allow_abbrev=False,
        description="Utility to load Senzing mapped JSON records and process redo records",
        formatter_class=argparse.RawTextHelpFormatter,
    )
    arg_parser.add_argument(
        "-f",
        "--file",
        default=[],
        metavar="file",
        nargs="*",
        help=textwrap.dedent(
            """\
            Path and name of file to load

            Default: None, skip loading but still process redo records

            """
        ),
    )
    arg_parser.add_argument(
        "-c",
        "--config-file",
        dest="config_file",
        help=textwrap.dedent(
            """\
            Optional path and file name of sz_engine_config.ini to use

            Default: None
            Env Var: SENZING_ENGINE_CONFIGURATION_JSON

            """
        ),
    )
    arg_parser.add_argument(
        "-w",
        "--with-info",
        action="store_true",
        default=False,
        dest="with_info",
        help=textwrap.dedent(
            """\
            Produce with info messages and write to a file

            Default: False

            """
        ),
    )
    arg_parser.add_argument(
        "-t",
        "--debug-trace",
        action="store_true",
        default=False,
        dest="debug_trace",
        help=textwrap.dedent(
            """\
            Output debug trace information

            Default: False

            """
        ),
    )
    arg_parser.add_argument(
        "-nt",
        "--num-threads",
        default=0,
        dest="num_threads",
        metavar="num_threads",
        type=int,
        help=textwrap.dedent(
            """\
            Total number of worker threads performing load

            Default: Calculated

            """
        ),
    )
    arg_parser.add_argument(
        "-n",
        "--no-redo",
        action="store_true",
        default=False,
        dest="no_redo",
        help=textwrap.dedent(
            f"""\
            Disable redo processing.
            Another instance of {MODULE_NAME} can be run in redo only mode or redo can be processed after ingestion

            Default: False

            """
        ),
    )
    arg_parser.add_argument(
        "-wp",
        "--withinfo-path",
        default="",
        dest="withinfo_path",
        metavar="path",
        help=textwrap.dedent(
            f"""\
            Alternative path for the with info file

            Default: {Path.cwd()}

            """
        ),
    )
    arg_parser.add_argument(
        "-ep",
        "--errors-path",
        default="",
        dest="errors_path",
        metavar="path",
        help=textwrap.dedent(
            f"""\
            Alternative path for the errors file

            Default: {Path.cwd()}

            """
        ),
    )
    arg_parser.add_argument(
        "-sp",
        "--shuffle-path",
        default="",
        dest="shuffle_path",
        metavar="path",
        help=textwrap.dedent(
            """\
            Alternative path for the shuffled file

            Default: Same path as file to load

            """
        ),
    )
    arg_parser.add_argument(
        "-rp",
        "--redirect-path",
        default="",
        dest="redirect_path",
        metavar="path",
        help=textwrap.dedent(
            f"""\
            Alternative path for the errors, shuffle and with info file. Overrides -ep, -wp and -sp

            Default: {Path.cwd()}

            """
        ),
    )
    arg_parser.add_argument(
        "-sfi",
        "--shuff-files-ignore",
        action="store_true",
        default=False,
        dest="shuff_files_ignore",
        help=textwrap.dedent(
            """\
            Skip checking for previously shuffled files and prompting to use them

            """
        ),
    )
    no_shuff_no_del = arg_parser.add_mutually_exclusive_group()
    no_shuff_no_del.add_argument(
        "-ns",
        "--no-shuffle",
        action="store_true",
        default=False,
        dest="no_shuffle",
        help=textwrap.dedent(
            """\
            Don\'t shuffle input file(s)
            Shuffling improves performance and shouldn\'t be disabled unless input file(s) are pre-shuffled

            """
        ),
    )
    no_shuff_no_del.add_argument(
        "-snd",
        "--shuffle-no-delete",
        action="store_true",
        default=False,
        dest="shuffle_no_delete",
        help=textwrap.dedent(
            f"""\
            Don\'t delete shuffled file(s) after {MODULE_NAME} shuffles them.
            Adds {SHUFF_NO_DEL_TAG} and timestamp to the shuffled file. {MODULE_NAME} can detect and reuse shuffled files

            """
        ),
    )
    arg_parser.add_argument(
        "-l",
        "--logging-output",
        action="store_true",
        default=False,
        dest="logging_output",
        help=textwrap.dedent(
            """\
            Use logging style output

            """
        ),
    )
    # For testing
    arg_parser.add_argument(
        "-sdsp",
        "--skip-ds-perf",
        action="store_true",
        default=False,
        dest="skip_ds_perf",
        help=argparse.SUPPRESS,
    )
    arg_parser.add_argument(
        "-rf",
        "--records-frequency",
        default=1000,
        dest="records_frequency",
        type=int,
        help=argparse.SUPPRESS,
    )
    arg_parser.add_argument(
        "-sf",
        "--stats-frequency",
        default=120,
        dest="stats_frequency",
        type=int,
        help=argparse.SUPPRESS,
    )

    return arg_parser.parse_args()


def check_ingest_files(files: list[str]):
    """TODO"""
    for file in files:
        json_errors = 0
        json_good = 0
        lines = []
        try:
            with open(file, "r", encoding="utf-8-sig") as ingest_file:
                for _ in range(1, 101):
                    read_line = ingest_file.readline().strip()
                    if read_line:
                        lines.append(read_line)

            for line in lines:
                try:
                    _ = _json_loads(line)
                    json_good += 1
                except JSONDecodeError:
                    json_errors += 1
                    # TODO
                    # try:
                    #     csv_sample = file_.read(5000)
                    #     csv_header = csv.Sniffer().has_header(csv_sample)
                    #     csv_dialect = csv.Sniffer().sniff(csv_sample)
                    #     print(f"\n{csv_header = }")
                    #     print(f"\n{csv_dialect.__dict__ = }")
                    #     reader = csv.reader(file_, csv_dialect)
                    #     for _ in range(500):
                    #         row = next(reader)
                    #         print(row)
                    # except Exception as err:
                    #     print(f"{err = }")
        except OSError as err:
            logger.info("")
            logger.error(err)
            sys.exit(1)

        if (json_errors > json_good) and json_good == 0:
            logger.info("")
            logger.error("File to ingest doesn't appear to be JSON. CSV files are not currently supported")
            sys.exit(1)


def docker_redirects(
    files_list: list[str],
    redirects,
    # redirect_path: str,
    # errors_path: str,
    # withinfo_path: str,
    # shuffle_path: str,
) -> tuple[Any, Any, Any]:
    """TODO"""
    errors_path = redirects.errors_path
    withinfo_path = redirects.withinfo_path
    shuffle_path = redirects.shuffle_path

    try:
        check_path_writeable(Path.cwd())
    except OSError:
        logger.info("")
        logger.info("Docker Checks")
        logger.info("-------------")
        logger.info("")
        logger.info("Detected running in Docker, CWD (%s) isn't writeable, checking redirect paths...", Path.cwd())

        # No file to get a path from and no redirects, if there are errors or with info for redo can't write
        if not files_list and not any(redirects):
            logger.info("No file to ingest, can't try and use its path to write error, shuffled, or with info files to")
            logger.info(
                "Specify -rp (or -ep, -wp, -sp) with a mount path to write error, shuffled, or with info files to, e.g., "
            )
            logger.info("")
            logger.info(
                "  docker run -u $UID -v ${PWD}:/my_data -e SENZING_ENGINE_CONFIGURATION_JSON senzing/sz_file_loader -f /my_data/customers.json -rp /my_data"
            )
            sys.exit(1)

        # Got overriding redirect path for all output files
        if redirects.redirect_path:
            logger.info("-rp specified, error, shuffled, and with info files will be written to the path specified")

        # No file to get a path from and no redirect, did get either -ep or -wp
        if not files_list and not redirects.redirect_path and any((errors_path, withinfo_path, shuffle_path)):
            logger.info("Setting missing error or with info output path to specified -ep or -wp")
            if not errors_path:
                errors_path = withinfo_path or shuffle_path
            if not withinfo_path:
                withinfo_path = errors_path or shuffle_path

        # Have a file but not another redirect
        if files_list and not any(redirects):
            file_parent = str(Path(files_list[0]).parent.resolve())
            logger.info(
                "Setting missing error, with info, shuffled files output paths to ingest file path, any of -ep, -wp, -sp, -rp not specified"
            )
            if not errors_path:
                errors_path = file_parent
            if not withinfo_path:
                withinfo_path = file_parent
            if not shuffle_path:
                shuffle_path = file_parent

        logger.info("")

    return (errors_path, withinfo_path, shuffle_path)


def check_redirect_paths(paths: Iterable[str]):
    """TODO"""
    try:
        for p in paths:
            if not p:
                continue
            check_path_writeable(p)
    except OSError as err:
        logger.info("")
        logger.error("Unable to write test file to redirect path: %s", err)
        sys.exit(1)


def shuffle_ingest_file(
    cli_args: argparse.Namespace, ingest_file: Path, shuffle_path: Union[Path, str]
) -> Union[None, Path]:
    """Shuffle a source file to reduce contention and improve performance"""
    # If source file was previously shuffled don't do it again
    if any(match in str(ingest_file) for match in [SHUFF_NO_DEL_TAG, SHUFF_TAG]):
        logger.info("")
        logger.info("Skipping shuffling, source file previously shuffled, file name contains %s", SHUFF_GLOB_TAG)
        return ingest_file

    # Add timestamp to shuffled file that shouldn't be deleted
    suffix = SHUFF_NO_DEL_TAG + datetime.now().strftime("%Y%m%d_%H-%M-%S") if cli_args.shuffle_no_delete else SHUFF_TAG
    shuff_file = Path(f"{ingest_file}{suffix}")
    shuff_file_glob = f"{ingest_file.name}{SHUFF_GLOB_TAG}"

    # Redirecting the shuffled file to a new path
    if shuffle_path:
        if isinstance(shuffle_path, str):
            shuffle_path = Path(shuffle_path)
        shuff_file = shuffle_path / shuff_file.name

    # Look for previously shuffled files in original path
    if not cli_args.shuff_files_ignore:
        prior_shuf_files = [p.resolve() for p in ingest_file.parent.glob(shuff_file_glob)]
        # If redirecting the shuffle file look for previously shuffled files there too
        if shuffle_path:
            prior_shuf_files.extend([p.resolve() for p in shuffle_path.glob(shuff_file_glob)])

        if prior_shuf_files:
            logger.info("")
            logger.info("Found previously shuffled files matching %s: ", ingest_file.name)
            prior_shuf_files.sort()
            for psf in prior_shuf_files:
                logger.info("  - %s    %s", psf, human_readable_bytes(Path(psf).stat().st_size))

            logger.info("")
            logger.info("The files above may not contain the same data as the input file!")
            logger.info("If quitting to use an above file, ensure the previously shuffled file is what you expect!")
            logger.info("")
            logger.info(
                "To quit and use a previously shuffled file hit <Enter>. To continue, wait %s seconds or type c",
                SHUFF_TIMEOUT,
            )

            # Wait to allow use of a prior shuffle, timeout and continue if automated
            while True:
                input_ = get_char_with_timeout(SHUFF_TIMEOUT)
                # If enter, stop and choose a previously shuffled file
                if input_ == "\n":
                    sys.exit(0)
                # If we timed out or hit c continue on
                elif not input_ or input_.lower() == "c":
                    break
                else:
                    logger.info("<Enter> to quit or c to continue...")

    logger.info("")
    logger.info("Shuffling to: %s", shuff_file)
    shuf_cmd = "gshuf" if sys.platform == "darwin" else "shuf"
    cmd = f"{shuf_cmd} {ingest_file} > {shuff_file}"
    # TODO
    # if sourceDict["FILE_FORMAT"] not in ("JSON", "UMF"):
    #     cmd = f"head -n1 {file_path} > {shuf_file_path} && tail -n+2 {file_path} | shuf >> {shuf_file_path}"

    try:
        _ = subprocess.run([cmd], capture_output=True, check=True, shell=True)
    except subprocess.CalledProcessError as err:
        logger.info("")
        logger.warning("Shuffling source file failed:")
        logger.warning("  Command: %s", cmd)
        logger.warning("  Error: %s", err.__dict__["stderr"].decode())
        logger.warning(
            "Continuing without shuffling. If performance appears low, fix the error or shuffle the file manually and restart the load"
        )
        time.sleep(5)
        return None

    shuff_file.chmod(ingest_file.stat().st_mode)

    return shuff_file


def get_sz_engines(
    sz_factory: SzAbstractFactoryCore,
) -> Tuple[SzEngine, SzDiagnostic, SzProduct, SzConfigManager]:
    """TODO"""
    try:
        sz_engine = sz_factory.create_engine()
        sz_diag = sz_factory.create_diagnostic()
        sz_product = sz_factory.create_product()
        sz_configmgr = sz_factory.create_configmanager()
    except SzError as err:
        raise err

    return (sz_engine, sz_diag, sz_product, sz_configmgr)


def prime_sz_engine(sz_engine: SzEngine) -> None:
    """TODO"""
    logger.info("")
    logger.info("Priming Senzing engine...")
    try:
        sz_engine.prime_engine()
    except SzError as err:
        raise err


def env_convert_boolean(env_var: str, cli_arg: bool) -> bool:
    """Convert boolean env var to True or False if set, otherwise use cli arg value"""
    evar = os.getenv(env_var)
    if evar:
        if isinstance(evar, str):
            if evar.lower() in ["true", "1", "t", "y", "yes"]:
                return True
            return False
        return evar

    return cli_arg


def startup_info(
    engine: SzEngine,
    diag: SzDiagnostic,
    configmgr: SzConfigManager,
    product: SzProduct,
    skip_ds_perf: bool,
) -> None:
    """Fetch and display information at startup."""
    try:
        lic_info = _json_loads(product.get_license())
        ver_info = _json_loads(product.get_version())
        config_list = _json_loads(configmgr.get_configs())
        active_cfg_id = engine.get_active_config_id()
        ds_info = _json_loads(diag.get_datastore_info())
    except SzError as err:
        raise err

    # Get details for the currently active config ID
    active_cfg_details = [details for details in config_list["CONFIGS"] if details["CONFIG_ID"] == active_cfg_id]
    config_comments = active_cfg_details[0]["CONFIG_COMMENTS"]
    config_created = active_cfg_details[0]["SYS_CREATE_DT"]

    # Get data store info, build list of strings, could be a cluster
    ds_list = []
    for ds in ds_info["dataStores"]:
        ds_list.append(f"{ds['id']} - {ds['type']} - {ds['location']}")

    logger.info("Arguments & Environment")
    logger.info("-----------------------")
    logger.info("")
    logger.info("%s", " ".join(sys.argv))

    logger.info("")
    logger.info("Version & Configuration")
    logger.info("-----------------------")
    logger.info("")
    logger.info(
        "Senzing Version:  %s (%s)",
        ver_info["VERSION"],
        ver_info["BUILD_DATE"],
    )
    logger.info("Config ID:        %s", active_cfg_id)
    logger.info("Config Comments:  %s", config_comments)
    logger.info("Config Created:   %s", config_created)
    logger.info("Datastore(s):     %s", ds_list.pop(0))
    for ds in ds_list:
        logger.info("%s%s", " " * 28, ds)
    logger.info("")
    logger.info("License")
    logger.info("-------")
    logger.info("")
    logger.info("Customer:    %s", lic_info["customer"])
    logger.info("Type:        %s", lic_info["licenseType"])
    logger.info("Records:     %s", lic_info["recordLimit"])
    logger.info("Expiration:  %s", lic_info["expireDate"])
    logger.info("Contract:    %s", lic_info["contract"])
    logger.info("")

    # Skip perf check if specified on CLI args or container env var
    if not skip_ds_perf:
        max_time_per_insert = 0.5
        db_tune_article = "https://senzing.zendesk.com/hc/en-us/articles/360016288254-Tuning-Your-Database"

        logger.info("")
        logger.info("Datastore Performance")
        logger.info("---------------------")
        logger.info("")

        ds_perf = _json_loads(diag.check_datastore_performance(3))
        num_recs_inserted = ds_perf.get("numRecordsInserted", None)
        if num_recs_inserted:
            insert_time = ds_perf["insertTime"]
            time_per_insert = (1.0 * insert_time / num_recs_inserted) if num_recs_inserted > 0 else 999
            logger.info("Records inserted:    %s", f"{num_recs_inserted:,}")
            logger.info("Period for inserts:  %s s", insert_time / 1000)
            logger.info("Average per insert:  %s ms", f"{time_per_insert:.3f}")
            logger.info("")
        else:
            logger.error("Datastore performance test failed!")

        if time_per_insert > max_time_per_insert:
            logger.warning(
                "Datastore performance of %s ms per insert is slower than the recommended minimum of %s ms per insert",
                f"{time_per_insert:.1f}",
                f"{max_time_per_insert:.1f}",
            )
            logger.warning("For database tuning refer to: %s", {db_tune_article})
            logger.warning("")
            logger.warning("Pausing for warning message...")
            logger.warning("")
            time.sleep(5)


def add_record(engine: SzEngine, rec_to_add: str, with_info: bool) -> Union[None, str]:
    """Add a single record, returning with info details if requested"""
    # Return "" if a blank line was read to prevent blank lines throwing errors when trying to json.loads()
    rec_to_add = rec_to_add.strip()
    if not rec_to_add:
        return None

    record_dict = _json_loads(rec_to_add)
    data_source = record_dict.get("DATA_SOURCE", "")
    record_id = record_dict.get("RECORD_ID", "")

    if with_info:
        response = engine.add_record(data_source, record_id, rec_to_add, SzEngineFlags.SZ_WITH_INFO)
        return response

    engine.add_record(data_source, record_id, rec_to_add)

    return ""


def get_redo_record(engine: SzEngine) -> str:
    """Get a redo record for processing"""
    try:
        redo_record = engine.get_redo_record()
    except SzError as err:
        logger.critical("Exception: %s - Operation: getRedoRecord", err)
        shutdown.set()
        return ""

    return redo_record


def prime_redo_records(engine: SzEngine, quantity: int) -> List[str]:
    """Get a specified number of redo records for priming processing"""
    redo_records = []
    for _ in range(quantity):
        single_redo_rec = get_redo_record(engine)
        if single_redo_rec:
            redo_records.append(single_redo_rec)

    return redo_records


def process_redo_record(engine: SzEngine, record: str, with_info: bool) -> str:
    """Process a single redo record, returning with info details if --info"""
    if with_info:
        response = engine.process_redo_record(record, SzEngineFlags.SZ_WITH_INFO)
        return response

    engine.process_redo_record(record)

    return ""


def record_stats(success_recs: int, error_recs: int, prev_time: float, operation: str) -> float:
    """Log details on records for add/redo"""
    logger.info(
        "Processed %s %s, %s records per second, %s errors",
        f"{success_recs:,}",
        operation,
        f"{int(1000 / (time.time() - prev_time)):,}",
        f"{error_recs:,}",
    )
    return time.time()


def workload_stats(engine: SzEngine) -> None:
    """Log engine workload stats"""
    try:
        logger.info("")
        logger.info(engine.get_stats())
        logger.info("")
    except SzError as err:
        logger.critical("Exception: %s - Operation: get_stats", err)
        shutdown.set()


def long_running_check(
    futures: dict[concurrent.futures.Future, tuple[str, float]],
    time_now: float,
    num_workers: int,
) -> None:
    """Check for long-running records"""
    num_stuck = 0
    for fut, payload in futures.items():
        if not fut.done():
            duration = time_now - payload[1]
            if duration > LONG_RECORD:
                num_stuck += 1
                stuck_record = _json_loads(payload[0])
                logger.warning(
                    "Long running record (%s): %s - %s",
                    f"{duration / 60:.3g}",
                    f"{stuck_record['DATA_SOURCE']}",
                    f"{stuck_record['RECORD_ID']}",
                )

    if num_stuck >= num_workers:
        logger.warning("All %s threads are stuck processing long running records", num_workers)


def signal_int(sig_num: int, stack_frame: FrameType) -> None:  # pylint: disable=unused-argument
    """Interrupt to allow running threads to finish"""
    logger.info("")
    logger.warning("Processing was interrupted, shutting down")
    logger.warning("Waiting for current tasks to complete, could take many minutes...")
    logger.info("")
    shutdown.set()


def load_and_redo(
    cli_args: argparse.Namespace,
    errors_file: Path,
    with_info_file: Path,
    sz_engine: SzEngine,
    with_info_out: TextIO,
    file_to_process: Union[None, TextIO] = None,
    ingest_file: Union[Path, None] = None,
    ingest_file_shuff: Union[Path, None] = None,
) -> dict[str, Any]:
    """Load records and process redo records after loading is complete"""

    def add_new_future(supplied_record: str = "") -> bool:
        """
        Add a new future as needed. supplied_record is used when a retryable error is caught to resend the
        record to add or redo
        True is returned if there are still records to process
        False is returned when no more records to process
        """
        if supplied_record:
            record = supplied_record
        else:
            if mode.__name__ == "add_record":
                record = file_to_process.readline()
            else:
                record = get_redo_record(sz_engine)

        if record:
            futures[executor.submit(mode, sz_engine, record, with_info)] = (record, time.time())
            return True

        return False

    error_recs = 0
    load_blank_lines = 0
    load_errors = 0
    load_success = 0
    load_time = 0.0
    no_redo = cli_args.no_redo
    num_workers = cli_args.num_threads
    redo_errors = 0
    redo_success = 0
    redo_time = 0.0
    with_info = cli_args.with_info
    no_shuffle = cli_args.no_shuffle
    recs_per_sec_output_frequency = cli_args.records_frequency
    shuffle_no_delete = cli_args.shuffle_no_delete
    stats_output_frequency = cli_args.stats_frequency

    # Test the max number of workers ThreadPoolExecutor allocates to use in sizing actual workers to request
    max_workers = num_workers if num_workers else get_max_futures_workers()

    modes = [add_record] if no_redo else [add_record, process_redo_record]
    main_start_time = time.time()

    for mode in modes:
        # If loading is stopped or fails don't process redo
        if shutdown.is_set():
            break

        add_future = True
        more_recs = False
        error_recs = 0
        long_check_time = time.time()
        prev_time = time.time()
        start_time = time.time()
        success_recs = 0
        work_stats_time = time.time()

        # If the file was empty or no file was specified skip loading
        if mode.__name__ == "add_record" and not file_to_process:
            logger.info("")
            logger.info("No input file. Skipping loading, checking for redo records...")
            continue

        # Prime add or redo based on thread pool max workers
        with concurrent.futures.ThreadPoolExecutor(max_workers) as executor:
            if mode.__name__ == "add_record":
                futures = {
                    executor.submit(mode, sz_engine, record, with_info): (record, time.time())
                    for record in itertools.islice(file_to_process, max_workers)
                }
            else:
                futures = {
                    executor.submit(mode, sz_engine, record, with_info): (record, time.time())
                    for record in prime_redo_records(sz_engine, max_workers)
                }
            logger.info("")
            logger.info("%s %s threads...", MODE_TEXT[mode.__name__]["start_msg"], max_workers)
            logger.info("")

            # Start processing add/redo
            while futures:
                done, _ = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)
                for f in done:
                    try:
                        result = f.result()
                    # TODO - Test on OS & SG load with redo when engine fixed for process_redo_record()
                    # TODO - Collect and retry later?
                    # # If caught a retryable error resubmit the record to try again
                    # except SzRetryableError as err:
                    #     logger.info("")
                    #     logger.info(
                    #         "Retrying record due to: %s - Operation: %s - Record: %s",
                    #         err,
                    #         mode_text[mode.__name__]["except_msg"],
                    #         futures[f][0].strip(),
                    #     )
                    #     logger.info("")
                    #     more_recs = add_new_future(futures[f][0].strip())
                    except (
                        SzError,
                        JSONDecodeError,
                    ) as err:
                        error_recs += 1
                        logger.info("")
                        logger.error(
                            "%s - Operation: %s - Record: %s",
                            err,
                            MODE_TEXT[mode.__name__]["except_msg"],
                            futures[f][0].strip(),
                        )
                        logger.info("")

                        if SzUnrecoverableError in type(err).mro():
                            shutdown.set()
                    else:
                        if add_future and not shutdown.is_set():
                            more_recs = add_new_future()

                        # If loading and None the line in the source was blank
                        if mode.__name__ == "add_record" and result is None:
                            load_blank_lines += 1
                        else:
                            # Write out with info result if it was requested
                            if result:
                                with_info_out.write(f"{result}\n")

                            success_recs += 1

                        if success_recs > 0 and success_recs % recs_per_sec_output_frequency == 0:
                            prev_time = record_stats(
                                success_recs,
                                error_recs,
                                prev_time,
                                MODE_TEXT[mode.__name__]["stats_msg"],
                            )
                    finally:
                        del futures[f]

                # Early errors check to catch mapping errors, missing dsrc_code, etc
                if error_recs == max_workers and success_recs == 0:
                    shutdown.set()

                # Wait until futures are complete if finishing up
                if (shutdown.is_set() or not more_recs) and len(futures) == 0:
                    break

                time_now = time.time()
                if time_now > work_stats_time + stats_output_frequency:
                    work_stats_time = time_now
                    workload_stats(sz_engine)

                if time_now > long_check_time + LONG_RECORD:
                    long_check_time = time_now
                    long_running_check(futures, time_now, max_workers)

        if not shutdown.is_set():
            workload_stats(sz_engine)

        # Store loading stats for overall results stats
        if mode.__name__ == "add_record":
            load_time = round(((time.time() - main_start_time) / 60), 1)
            load_errors = error_recs
            load_success = success_recs
            if not shutdown.is_set():
                logger.info(
                    "Successfully loaded %s records in %s mins with %s error(s)",
                    f"{load_success:,}",
                    load_time,
                    f"{load_errors:,}",
                )
        else:
            redo_time = 0 if no_redo else round((time.time() - start_time) / 60, 1)
            redo_errors = error_recs
            redo_success = success_recs

    results = {
        "source_file": str(ingest_file) if ingest_file else None,
        "persisted_shuff_file": (ingest_file_shuff if shuffle_no_delete else None),
        "did_shuff": bool(not no_shuffle and ingest_file_shuff),
        "errors_file": (str(errors_file.resolve()) if (load_errors + error_recs) > 0 else None),
        "with_info": (str(with_info_file.resolve()) if with_info else None),
        "elapsed_time_total": load_time + redo_time,
        "blank_lines": load_blank_lines,
        "load_stats": {
            "success_recs": load_success,
            "error_recs": load_errors,
            "elapsed_time": load_time,
        },
        "redo_stats": {
            "success_recs": redo_success if not no_redo else 0,
            "error_recs": redo_errors if not no_redo else 0,
            "elapsed_time": redo_time if not no_redo else 0,
        },
    }

    return results


def per_result(cli_args: argparse.Namespace, result: dict[str, Any]) -> None:
    """TODO"""
    logger.info("")
    logger.info("Results")
    logger.info("-------")
    logger.info("")

    if result["source_file"]:
        logger.info("Source file:                %s", result["source_file"])
        if cli_args.shuffle_no_delete and result["persisted_shuff_file"]:
            logger.info("Persisted shuffled file:    %s", result["persisted_shuff_file"])
        else:
            did_shuf = "Yes" if result["did_shuff"] else "No"
            logger.info("Source file shuffled:       %s", did_shuf)
        logger.info("Source file blank lines:    %s", result["blank_lines"])
    logger.info(
        "With info file:             %s",
        result["with_info"] if result["with_info"] else "Not requested",
    )
    logger.info(
        "Errors file:                %s",
        result["errors_file"] if result["errors_file"] else "No errors",
    )

    logger.info("")
    if result["source_file"]:
        logger.info("Successful load records:    %s", f"{result['load_stats']['success_recs']:,}")
        logger.info("Error load records:         %s", f"{result['load_stats']['error_recs']:,}")
        logger.info("Loading elapsed time (m):   %s", f"{result['load_stats']['elapsed_time']:,.1f}")

    logger.info("")
    if not cli_args.no_redo:
        logger.info("Successful redo records:    %s", f"{result['redo_stats']['success_recs']:,}")
        logger.info("Error redo records:         %s", f"{result['redo_stats']['error_recs']:,}")
        logger.info("Redo elapsed time (m):      %s", f"{result['redo_stats']['elapsed_time']:,.1f}")
    else:
        logger.info("Redo:                       %s", "Not requested")
    logger.info("")

    if result["source_file"] and not cli_args.no_redo:
        logger.info(
            "Total elapsed time (m):     %s",
            f"{result['load_stats']['elapsed_time'] + result['redo_stats']['elapsed_time']:,.1f}",
        )
        logger.info("")


def summary_results(
    cli_args: argparse.Namespace,
    overall_results: dict[str, dict[str, Any]],
) -> None:
    """TODO"""
    elapsed_time_total = 0
    load_blank_lines_total = 0
    load_error_total = 0
    load_success_total = 0
    load_time_total = 0
    redo_error_total = 0
    redo_success_total = 0
    redo_time_total = 0

    for result in overall_results.values():
        load_success_total += result["load_stats"]["success_recs"]
        load_error_total += result["load_stats"]["error_recs"]
        load_blank_lines_total += result["blank_lines"]
        load_time_total += result["load_stats"]["elapsed_time"]
        redo_success_total += result["redo_stats"]["success_recs"]
        redo_error_total += result["redo_stats"]["error_recs"]
        redo_time_total += result["redo_stats"]["elapsed_time"]
        elapsed_time_total += result["elapsed_time_total"]

    logger.info("")
    logger.info("Overall Results")
    logger.info("---------------")
    logger.info("")
    logger.info("Files processed:  %s", len(overall_results))
    logger.info("Empty lines:      %s", load_blank_lines_total)
    if cli_args.shuffle_no_delete:
        logger.info(
            "Files shuffled:   %s",
            "Yes" if overall_results["1"]["did_shuff"] else "No",
        )
    logger.info(
        "With info file:   %s",
        (overall_results["1"]["with_info"] if overall_results["1"]["with_info"] else "Not requested"),
    )
    logger.info(
        "Errors file:      %s",
        (overall_results["1"]["errors_file"] if overall_results["1"]["errors_file"] else "No errors"),
    )
    logger.info("")
    logger.info("Loaded records:   %s", load_success_total)
    logger.info("Error records:    %s", load_error_total)
    logger.info("Load time (s):    %s", load_time_total)
    logger.info("")

    if not cli_args.no_redo:
        logger.info("Redo records:     %s", redo_success_total)
        logger.info("Error records:    %s", redo_error_total)
        logger.info("Redo time (s):    %s", redo_time_total)
        logger.info("")
        logger.info("Total time (s):   %s", elapsed_time_total)
    else:
        logger.info("Redo:             %s", "Not requested")
    logger.info("")


def main():
    """main"""
    signal.signal(signal.SIGINT, signal_int)

    cli_args = parse_cli_args()
    if not cli_args.file and cli_args.no_redo:
        logger.info("")
        logger.error("No input file and redo processing disabled, nothing to do!")
        sys.exit(1)

    errors_file = Path(f"{MODULE_NAME}_errors_{START_TS}.log")
    errors_path = cli_args.errors_path
    files_list = cli_args.file
    overall_results: dict[str, dict[str, Any]] = {}
    redirect_path = cli_args.redirect_path
    shuffle_path = cli_args.shuffle_path
    with_info_file = Path(f"{MODULE_NAME}_with_info_{START_TS}.jsonl")
    withinfo_path = cli_args.withinfo_path
    Redirects = namedtuple("Redirects", "errors_path redirect_path withinfo_path shuffle_path")
    redirects = Redirects(errors_path, redirect_path, withinfo_path, shuffle_path)

    if in_docker():
        errors_path, withinfo_path, shuffle_path = docker_redirects(
            files_list,
            redirects,
        )

    if redirect_path:
        errors_file = Path(redirect_path) / errors_file.name
        with_info_file = Path(redirect_path) / with_info_file.name
        shuffle_path = Path(shuffle_path) if shuffle_path else shuffle_path
    else:
        if errors_path:
            errors_file = Path(errors_path) / errors_file
        if withinfo_path:
            with_info_file = Path(withinfo_path) / with_info_file

    check_ingest_files(files_list)
    check_redirect_paths((redirects))

    atexit.register(files_clean_up, errors_file, with_info_file)

    # Have errors file if redirected, add file handler to logger and chmod
    try:
        if cli_args.logging_output:
            console_handle.setFormatter(logging.Formatter(LOG_FORMAT))
        file_handle = logging.FileHandler(errors_file, "w")
        file_handle.setLevel(logging.ERROR)
        file_handle.setFormatter(logging.Formatter(LOG_FORMAT))
        logger.addHandler(file_handle)
        errors_file.chmod(0o660)
    except OSError as err:
        print(f"\nERROR: Couldn't create logger file: {err}")
        sys.exit(1)

    # Create engines and display startup information
    try:
        engine_config = get_engine_config(cli_args.config_file)
        sz_factory = SzAbstractFactoryCore(MODULE_NAME, engine_config, verbose_logging=cli_args.debug_trace)
        sz_engine, sz_diag, sz_product, sz_configmgr = get_sz_engines(sz_factory)
        startup_info(sz_engine, sz_diag, sz_configmgr, sz_product, cli_args.skip_ds_perf)
        prime_sz_engine(sz_engine)
    except SzError as err:
        logger.error(err)
        sys.exit(1)

    # Start loading any file(s) specified
    with open(with_info_file, "w", encoding="utf-8") as with_info_out:
        with_info_file.chmod(0o660)

        for idx, ingest_file in enumerate(files_list, start=1):
            ingest_file = Path(ingest_file).resolve()
            ingest_file_shuff = None

            if idx > 1:
                logger.info("")
            logger.info("")
            logger.info("-" * 100)
            logger.info("Processing: %s", ingest_file)

            if not cli_args.no_shuffle:
                if ingest_file.stat().st_size > 50_000:
                    ingest_file_shuff = shuffle_ingest_file(cli_args, ingest_file, shuffle_path)
                else:
                    logger.info("")
                    logger.info("Not shuffling the file, small file size")

            ingest_or_shuff_file = ingest_file_shuff if ingest_file_shuff else ingest_file

            with open(ingest_or_shuff_file, "r", encoding="utf-8-sig") as file_to_process:
                results = load_and_redo(
                    cli_args,
                    errors_file,
                    with_info_file,
                    sz_engine,
                    with_info_out,
                    file_to_process,
                    ingest_file,
                    ingest_file_shuff,
                )

            per_result(cli_args, results)
            overall_results[str(idx)] = results

            if shutdown.is_set():
                print("\nERROR: An error occurred that caused processing to stop, please check the error log file.")
                break

            # Remove shuffled file if ingest file was shuffled and shuffle wasn't disabled
            if (
                ingest_file_shuff
                and ingest_file != ingest_file_shuff
                and not cli_args.shuffle_no_delete
                and not cli_args.no_shuffle
            ):
                with suppress(OSError):
                    Path.unlink(ingest_file_shuff)

        # If no files were specified perform redo
        if not files_list and not cli_args.no_redo:
            results = load_and_redo(cli_args, errors_file, with_info_file, sz_engine, with_info_out)
            per_result(cli_args, results)
            overall_results["redo_only"] = results

    if len(overall_results) > 1:
        summary_results(cli_args, overall_results)


if __name__ == "__main__":
    main()
