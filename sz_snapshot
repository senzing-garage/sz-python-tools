#! /usr/bin/env python3

import argparse
import csv
import json
import os
import random
import signal
import sys
import textwrap
import time
from datetime import datetime

from _tool_helpers import get_engine_config, get_max_futures_workers
from senzing import SzConfigManager, SzEngine, SzEngineFlags, SzError, SzProduct
from sz_database import SzDatabase
try:
    import orjson
except:
    orjson = None

import multiprocessing
from queue import Empty, Full

class IOQueueProcessor():

    def __init__(self, input_class, output_class, **kwargs):

        self.process_count = kwargs.get('process_count', multiprocessing.cpu_count())
        self.all_stop = multiprocessing.Value('i', 0)

        self.input_class = input_class
        self.output_class = output_class
        self.input_queue = multiprocessing.Queue(self.process_count * 10)
        self.output_queue = multiprocessing.Queue(self.process_count * 10)

        self.input_queue_read_cnt = multiprocessing.Value('i', 0)
        self.output_queue_read_cnt = multiprocessing.Value('i', 0)

        self.kwargs = kwargs
        self.process_list = []

    def start_up(self):
        self.process_list.append(multiprocessing.Process(target=self.output_queue_reader, args=(0, self.output_queue, self.output_class), kwargs=self.kwargs))
        for process_number in range(self.process_count - 1):
            self.process_list.append(multiprocessing.Process(target=self.input_queue_reader, args=(process_number+1, self.input_queue, self.output_queue, self.input_class), kwargs=self.kwargs))
        for process in self.process_list:
            process.start()

    def wait_for_queues(self, max_wait = 2):
        waits = 0
        while self.input_queue.qsize() or self.output_queue.qsize():
            size1 = self.input_queue.qsize()
            size2 = self.output_queue.qsize()
            print(f"waiting for {size1} input and {size2} output queue records")
            time.sleep(1)
            waits += 1
            if max_wait and waits > max_wait:
                break
        if self.input_queue.qsize() or self.output_queue.qsize():
            print('WARNING: Queues not empty!')

    def finish_up(self):
        self.wait_for_queues()
        with self.all_stop.get_lock():
            self.all_stop.value = 1

        start = time.time()
        while time.time() - start <= 15:
            if not any(process.is_alive() for process in self.process_list):
                break
            time.sleep(1)

        for process in self.process_list:
            if process.is_alive():
                print(process.name, 'did not terminate gracefully')
                process.terminate()
            process.join()

        self.input_queue.close()
        self.output_queue.close()

    def queue_read(self, q):
        try:
            return q.get(True, 1)
        except Empty:
            return None

    def queue_write(self, q, msg):
        while True:
            try:
                q.put(msg, True, 1)
            except Full:
                continue
            break

    def input_queue_reader(self, process_number, input_queue, output_queue, function_ref, **kwargs):
        kwargs['process_number'] = process_number
        input_class = function_ref(**kwargs)

        while self.all_stop.value == 0:
            queue_data = self.queue_read(input_queue)
            if queue_data:
                with self.input_queue_read_cnt.get_lock():
                    self.input_queue_read_cnt.value += 1
                result = input_class.run(queue_data)
                if result:
                    self.queue_write(output_queue, result)

        input_class.close()

    def output_queue_reader(self, process_number, output_queue, function_ref, **kwargs):

        kwargs['process_number'] = process_number
        output_class = function_ref(**kwargs)

        while self.all_stop.value == 0:
            queue_data = self.queue_read(output_queue)
            if queue_data:
                with self.output_queue_read_cnt.get_lock():
                    self.output_queue_read_cnt.value += 1
                output_class.run(queue_data)

        output_class.close()

    def process(self, msg):
        self.queue_write(self.input_queue, msg)

    def signal_writer(self, msg):
        self.queue_write(self.output_queue, msg)

    def get_input_queue_read_cnt(self):
        return self.input_queue_read_cnt.value

    def get_output_queue_read_cnt(self):
        return self.output_queue_read_cnt.value


class reader():

    def __init__(self, **kwargs):
        
        self.sz_config_data = kwargs.get("sz_config_data")
        self.relationship_filter = kwargs.get("relationship_filter")

        self.sz_db_uri = kwargs.get("sz_db_uri")
        self.api_version = kwargs.get("api_version")
        self.sz_config_data = kwargs.get("sz_config_data")
        self.dsrc_lookup = {item["DSRC_ID"]: item for item in sz_config_data["G2_CONFIG"]["CFG_DSRC"]}
        self.dsrc_code_lookup = {item["DSRC_CODE"]: item for item in sz_config_data["G2_CONFIG"]["CFG_DSRC"]}
        self.errule_lookup = {item["ERRULE_ID"]: item for item in sz_config_data["G2_CONFIG"]["CFG_ERRULE"]}
        self.errule_code_lookup = {item["ERRULE_CODE"]: item for item in sz_config_data["G2_CONFIG"]["CFG_ERRULE"]}
        self.errule_code_lookup["DISCLOSED"] = {"ERRULE_ID": 0}
        self.ftype_lookup = {item["FTYPE_ID"]: item for item in sz_config_data["G2_CONFIG"]["CFG_FTYPE"]}
        self.ftype_code_lookup = {item["FTYPE_CODE"]: item for item in sz_config_data["G2_CONFIG"]["CFG_FTYPE"]}

        self.sz_dbo = SzDatabase(self.sz_db_uri)
        sql_entities = (
            "select " 
            " a.RES_ENT_ID as RESOLVED_ENTITY_ID, "
            " a.ERRULE_ID, "
            " a.MATCH_KEY, "
            " b.DSRC_ID, "
            " c.RECORD_ID "
            "from RES_ENT_OKEY a "
            "join OBS_ENT b on b.OBS_ENT_ID = a.OBS_ENT_ID "
            "join DSRC_RECORD c on c.ENT_SRC_KEY = b.ENT_SRC_KEY and c.DSRC_ID = b.DSRC_ID "
            "where a.RES_ENT_ID = ?"
        )
        self.sql_entities = sz_dbo.sqlPrep(sql_entities)
        sql_relations = (
            "select "
            " a.RES_ENT_ID as RESOLVED_ENTITY_ID, "
            " a.REL_ENT_ID as RELATED_ENTITY_ID, "
            " b.LAST_ERRULE_ID as ERRULE_ID, "
            " b.IS_DISCLOSED, "
            " b.IS_AMBIGUOUS, "
            " b.MATCH_KEY, "
            " d.DSRC_ID "
            "from RES_REL_EKEY a "
            "join RES_RELATE b on b.RES_REL_ID = a.RES_REL_ID "
            "join RES_ENT_OKEY c on c.RES_ENT_ID = a.REL_ENT_ID "
            "join OBS_ENT d on d.OBS_ENT_ID = c.OBS_ENT_ID "
            "where a.RES_ENT_ID = ?"
        )
        self.sql_relations = sz_dbo.sqlPrep(sql_relations)

    def old_init(self):
        dsrc_lookup = {}
        dsrc_lookup_by_code = {}
        # TODO - Ant - SZ_CONFIG? Jira
        for cfg_record in config_data["G2_CONFIG"]["CFG_DSRC"]:
            dsrc_lookup[cfg_record["DSRC_ID"]] = cfg_record
            dsrc_lookup_by_code[cfg_record["DSRC_CODE"]] = cfg_record

        errule_lookup = {}
        errule_lookup_2 = {}
        for cfg_record in config_data["G2_CONFIG"]["CFG_ERRULE"]:
            errule_lookup[cfg_record["ERRULE_ID"]] = cfg_record
            errule_lookup_2[cfg_record["ERRULE_CODE"]] = cfg_record

        AMBIGUOUS_FTYPE_ID = 0
        esb_ftype_lookup = {}
        for cfg_record in config_data["G2_CONFIG"]["CFG_FTYPE"]:
            if cfg_record["DERIVED"] == "No":
                esb_ftype_lookup[cfg_record["FTYPE_ID"]] = cfg_record
            if cfg_record["FTYPE_CODE"] == "AMBIGUOUS_ENTITY":
                AMBIGUOUS_FTYPE_ID = cfg_record["FTYPE_ID"]
        
    def close(self):
        self.sz_dbo.close()

    def run(self, entity_id):
        resume_rows = []
        cursor = self.sz_dbo.sqlExec(self.sql_entities, [entity_id,])
        for row_data in self.sz_dbo.fetchAllDicts(cursor):
            row_data = self.complete_resume_db(row_data)
            resume_rows.append(row_data)
        if resume_rows and self.relationship_filter in (2, 3):
            cursor = self.sz_dbo.sqlExec(self.sql_relations, [entity_id,])
            for row_data in self.sz_dbo.fetchAllDicts(cursor):
                row_data = self.complete_resume_db(row_data)
                resume_rows.append(row_data)
        return resume_rows

    def complete_resume_db(self, row_data):
        if "RELATED_ENTITY_ID" not in row_data:
            row_data["RELATED_ENTITY_ID"] = 0
            row_data["IS_DISCLOSED"] = 0
            row_data["IS_AMBIGUOUS"] = 0
        if "RECORD_ID" not in row_data:
            row_data["RECORD_ID"] = "n/a"
        row_data["DATA_SOURCE"] = self.dsrc_lookup[row_data["DSRC_ID"]]["DSRC_CODE"]
        if not row_data['ERRULE_ID']:
            row_data["ERRULE_CODE"] = "unk"
        else:             
            row_data["ERRULE_CODE"] =self. errule_lookup[row_data["ERRULE_ID"]]["ERRULE_CODE"]

        if row_data["RELATED_ENTITY_ID"] == 0:
            row_data["MATCH_LEVEL"] = 1 if row_data["ERRULE_CODE"] else 0
        elif row_data["IS_DISCLOSED"] != 0:
            row_data["MATCH_LEVEL"] = 11
        else:
            row_data["MATCH_LEVEL"] = self.errule_lookup[row_data["ERRULE_ID"]]["RTYPE_ID"]
        return row_data


class writer():

    def __init__(self, **kwargs):
        self.stat_pack = kwargs.get("stat_pack")
        self.sample_size = kwargs.get("sample_size")
        self.export_csv = kwargs.get("for_audit")
        self.stats_file_name = kwargs.get("stats_file_name")
        self.csv_file_name = kwargs.get("csv_file_name")

    def close(self):
        return None

    def run(self, queue_data):
        if isinstance(queue_data, list):
            self.compute_stats(queue_data)
        else:
            print(f'updated stats')
            self.stat_pack["PROCESS"].update(queue_data)
            with open(self.stats_file_name, "w", encoding="utf-8") as outfile:
                json.dump(self.stat_pack, outfile, indent=4)

    def get_random_index(self):
        target_index = int(self.sample_size * random.random())
        if target_index % 10 != 0:
            return target_index
        return 0

    def update_stat_pack(self, prior_keys, stats):
        d = self.stat_pack
        for key in prior_keys:
            if not d.get(key):
                d[key] = {}
            d = d[key] 
        for k, v in stats.items():
            if k not in d:
                d[k] = v            
            else:
                d[k] += v

    def compute_stats(self, resume_rows):
        record_list = []
        resume_data = {}
        entity_size = 0
        entity_id = resume_rows[0]["RESOLVED_ENTITY_ID"]
        self.random_index = self.get_random_index() if entity_id % 10 == 0 else 0
        for row_data in resume_rows:
            related_id = row_data["RELATED_ENTITY_ID"]
            data_source = row_data["DATA_SOURCE"]
            record_id = row_data["RECORD_ID"]
            principle = f"{row_data['ERRULE_ID']}: {row_data['ERRULE_CODE']}" if row_data["ERRULE_ID"] else ""
            match_key = row_data["MATCH_KEY"]
            if related_id == 0:
                match_level = "MATCH"
                entity_size += 1
                record_list.append(data_source + ":" + record_id)
            elif row_data["IS_DISCLOSED"] != 0:
                match_level = "DISCLOSED_RELATION"
                principle = "DISCLOSURE"
            elif row_data["IS_AMBIGUOUS"] != 0:
                match_level = "AMBIGUOUS_MATCH"
            elif row_data["MATCH_LEVEL"] == 2:
                match_level = "POSSIBLE_MATCH"
            else:
                match_level = "POSSIBLY_RELATED"

            if related_id not in resume_data:
                resume_data[related_id] = {}
                resume_data[related_id]["MATCH_LEVEL"] = match_level
                resume_data[related_id]["DATA_SOURCES"] = {}
                resume_data[related_id]["PRINCIPLES"] = {}
                
            if data_source not in resume_data[related_id]["DATA_SOURCES"]:
                resume_data[related_id]["DATA_SOURCES"][data_source] = {"COUNT": 1, "PRINCIPLES": []}
            else:
                resume_data[related_id]["DATA_SOURCES"][data_source]["COUNT"] += 1

            if principle:
                principle_matchkey = f"{principle}||{match_key}"
                if principle_matchkey not in resume_data[related_id]["DATA_SOURCES"][data_source]["PRINCIPLES"]:
                    resume_data[related_id]["DATA_SOURCES"][data_source]["PRINCIPLES"].append(principle_matchkey)
                if related_id == 0:
                    if principle_matchkey not in resume_data[related_id]["PRINCIPLES"]: 
                        resume_data[related_id]["PRINCIPLES"][principle_matchkey] = 1
                    else:
                        resume_data[related_id]["PRINCIPLES"][principle_matchkey] += 1
                    



        self.update_stat_pack(["TOTALS"], {'ENTITY_COUNT': 1})
        self.update_stat_pack(["TOTALS"], {'RECORD_COUNT': entity_size})
        self.update_stat_pack(['ENTITY_SIZES', entity_size], {'COUNT': 1, 'SAMPLE': [entity_id]})

        print('-' * 10)
        print(json.dumps(resume_data, indent=4))

        entity0_sources = None # should get set as entity list is sorted
        for related_id in sorted(resume_data.keys()):
            match_level = resume_data[related_id]["MATCH_LEVEL"]
            if related_id == 0:
                for principle_matchkey in resume_data[related_id]["PRINCIPLES"]:
                    record_count = resume_data[related_id]["PRINCIPLES"][principle_matchkey]
                    stat_keys = ["TOTALS", match_level, principle_matchkey]
                    self.update_stat_pack(stat_keys, {"ENTITY_COUNT": 1, "RECORD_COUNT": record_count, "SAMPLE": [entity_id]})
                    
                entity0_sources = resume_data[related_id]["DATA_SOURCES"] 
                for data_source1 in entity0_sources:
                    record_cnt = entity0_sources[data_source1]["COUNT"]
                    for data_source2 in entity0_sources:
                        if data_source2 == data_source1:
                            stat_keys = ["DATA_SOURCE", data_source1]
                            self.update_stat_pack(stat_keys, {"ENTITY_COUNT": 1, "RECORD_COUNT": record_cnt})
                            if record_cnt > 1:
                                stat_keys.append(match_level)
                                self.update_stat_pack(stat_keys, {"ENTITY_COUNT": 1, "RECORD_COUNT": record_cnt})
                                if len(entity0_sources[data_source1]["PRINCIPLES"]) == 1:
                                    principle_matchkey = entity0_sources[data_source1]["PRINCIPLES"][0]
                                elif len(entity0_sources[data_source1]["PRINCIPLES"]) > 1:
                                    principle_matchkey = "multiple||multiple"
                                else:
                                    principle_matchkey = "none||none"
                                stat_keys.extend(["PRINCIPLES", principle_matchkey])
                                self.update_stat_pack(stat_keys, {'COUNT': 1, 'SAMPLE': [entity_id]})
                        else:
                            stat_keys = ["CROSS_SOURCE", f"{data_source1}||{data_source2}", match_level]
                            self.update_stat_pack(stat_keys, {"ENTITY_COUNT": 1, "RECORD_COUNT": record_cnt})
                            if len(entity0_sources[data_source2]["PRINCIPLES"]) == 1:
                                principle_matchkey = entity0_sources[data_source2]["PRINCIPLES"][0]
                            elif len(entity0_sources[data_source2]["PRINCIPLES"]) > 1:
                                principle_matchkey = "multiple||multiple"
                            elif len(entity0_sources[data_source1]["PRINCIPLES"]) == 1:
                                principle_matchkey = entity0_sources[data_source1]["PRINCIPLES"][0]
                            else:
                                principle_matchkey = "indeterminate||indeterminate"
                            stat_keys.extend(["PRINCIPLES", principle_matchkey])
                            self.update_stat_pack(stat_keys, {'COUNT': 1, 'SAMPLE': [entity_id]})
                if len(entity0_sources) > 1:
                    multi_source_key = "||".join(sorted(entity0_sources.keys()))
                    self.update_stat_pack(["MULTI_SOURCE", multi_source_key], {'COUNT': 1, 'SAMPLE': [entity_id]})

                                                        
                                                        
def initialize_stat_pack(stats_file_name, csv_file_name, quiet):
    new_stat_pack = True
    stats_file_existed = os.path.exists(stats_file_name)
    if stats_file_existed:
        stat_pack = json.load(open(stats_file_name))
        prior_status = "Unknown"
        last_entity_id = 0
        if stat_pack.get("PROCESS"):
            prior_status = stat_pack["PROCESS"]["STATUS"]
            last_entity_id = stat_pack["PROCESS"].get("LAST_ENTITY_ID", 0)
            entity_count = stat_pack["TOTALS"]["ENTITY_COUNT"]
        print(f'\n{prior_status} snapshot file exists with {entity_count} entities processed!')
        if quiet:
            print("PRIOR FILES WILL BE OVERWRITTEN")
        else:
            if prior_status != "Complete" and last_entity_id != 0:
                ans = input("\nDo you want to pick up where it left off? (y/n) ")
                if ans.upper().startswith("Y"):
                    new_stat_pack = False
            if new_stat_pack:
                ans = input("\nAre you sure you want to overwrite it? (y/n) ")
                if not ans.upper().startswith("Y"):
                    return None
            print()
    if new_stat_pack and os.path.exists(csv_file_name):
        os.remove(csv_file_name)
    if new_stat_pack:
        stat_pack = {
            "SOURCE": "sz_snapshot",
            "PROCESS": {
                "STATUS": "Incomplete",
                "START_TIME": datetime.now().strftime("%m/%d/%Y %H:%M:%S"),
                "LAST_ENTITY_ID": 0
            },
            "TOTALS": {},
            "DATA_SOURCE": {},
            "CROSS_SOURCE": {},
            "MULTI_SOURCE": {},
            "ENTITY_SIZES": {}
        }
    return stat_pack

def get_entity_sql(sz_dbo, data_source_filter):
    print("Determining entity range ...")
    if not datasource_filter:
        max_sql = "select max(RES_ENT_ID) from RES_ENT"
        min_entity_id = 0
        max_entity_id = sz_dbo.fetchRow(sz_dbo.sqlExec(max_sql))[0]
        entity_sql = "select RES_ENT_ID from RES_ENT where RES_ENT_ID between ? and ?"
    else:
        max_sql = (
            "select  "
            " min(b.RES_ENT_ID), "
            " max(b.RES_ENT_ID) "
            "from OBS_ENT a "
            "join RES_ENT_OKEY b on b.OBS_ENT_ID = a.OBS_ENT_ID "
            "where a.DSRC_ID = " + data_source_filter
        )
        min_entity_id, max_entity_id = sz_dbo.fetchRow(g2Dbo.sqlExec(sql))
        entity_sql = (
            "select distinct"
            " a.RES_ENT_ID "
            "from RES_ENT_OKEY a "
            "join OBS_ENT b on b.OBS_ENT_ID = a.OBS_ENT_ID "
            "where a.RES_ENT_ID between ? and ? and b.DSRC_ID = "
            + str(datasourceFilterID)
        )
    return entity_sql, min_entity_id, max_entity_id


def debug_print(_value, _desc="some variable"):
    print("-" * 20)
    print(_desc)
    if type(_value) in (list, dict):
        print(json.dumps(_value, indent=4))
    else:
        print(_value)
    input("press any key ...")
        
def print_exception_info():
    print(traceback.format_exc())

def signal_handler(signal, frame):
    global shut_down
    shut_down = 9


if __name__ == "__main__":
    shut_down = 0
    signal.signal(signal.SIGINT, signal_handler)

    PROGRESS_INTERVAL = 10000

    output_file_root = os.getenv("SENZING_OUTPUT_FILE_ROOT", None)
    env_size = os.getenv("SENZING_SAMPLE_SIZE", "")
    sample_size = int(env_size) if env_size and env_size.isdigit() else 1000
    datasource_filter = os.getenv("SENZING_DATASOURCE_FILTER", None)
    env_filter = os.getenv("SENZING_RELATIONSHIP_FILTER", None)
    relationship_filter = int(env_filter) if env_filter and env_filter.isdigit() else 3
    env_chunk = os.getenv("SENZING_CHUNK_SIZE", None)
    chunk_size = int(env_chunk) if env_chunk and env_chunk.isdigit() else 1000000
    env_thread = os.getenv("SENZING_THREAD_COUNT", None)
    thread_count = int(env_thread) if env_thread and env_thread.isdigit() else 0
    argParser = argparse.ArgumentParser()
    argParser.add_argument("-o", "--output_file_root", default=output_file_root, help='root name for files to be created')
    argParser.add_argument("-c", "--config_file_name", help="Path and name of optional G2Module.ini file to use.")
    argParser.add_argument("-s", "--sample_size", type=int, default=sample_size, help="the number of samples to log")
    argParser.add_argument("-d", "--dsrc_filter", help="optional data source code to analayze")
    argParser.add_argument("-f","--relationship_filter", type=int, default=relationship_filter, 
        help="filter options 1=No Relationships, 2=Include possible matches, 3=Include possibly related and disclosed")
    argParser.add_argument("-a", "--for_audit", action="store_true", default=False, help="export csv file for audit")
    argParser.add_argument("-k", "--chunk_size", type=int, default=chunk_size, help="records per batch")
    argParser.add_argument("-t", "--thread_count", type=int, default=thread_count, help="number of threads to start")
    argParser.add_argument("-u", "--use_api", action="store_true", default=False, help="use export api instead of sql to perform snapshot")
    argParser.add_argument("-q", "--quiet", action="store_true", default=False, help="doesn't ask to confirm before overwrite files")
    args = argParser.parse_args()
    print()
    
    if not args.output_file_root:
        print("Please use -o to select an output path and root file name such as /project/audit/snap1")
        sys.exit(1)

    try:
        sz_api_config = get_engine_config(args.config_file_name)
        sz_db_uri = json.loads(sz_api_config)["SQL"]["CONNECTION"]
        sz_engine = SzEngine("sz_snapshot", sz_api_config, False)
        sz_product = SzProduct()
        api_version = json.loads(sz_product.get_version())
        api_version_major = int(api_version["VERSION"][0:1])       
        sz_config_mgr = SzConfigManager("sz_explorer_c", sz_api_config, False)
        sz_config_data = json.loads(sz_config_mgr.get_config(sz_config_mgr.get_default_config_id()))
    except SzError as err:
        print_exception_info
        sys.exit(1)

    try:
        sz_dbo = SzDatabase(sz_db_uri)
    except Exception as err:
        print(f"\n{err}")
        sz_dbo = None
        print('Gonna have to make it use the api one day')
        sys.exit(0)

    dsrc_id_filter = None
    if args.dsrc_filter:
        dsrc_code_lookup = {item["DSRC_CODE"]: item for item in sz_config_data["G2_CONFIG"]["CFG_DSRC"]}
        if not dsrc_code_lookup.get(args.dsrc_filter):
            print(f"Data source code {args.dsrc_filter} is not valid\n")
            sys.exit(1)
        else:
            dsrc_id_filter = dsrc_code_lookup[args.dsrc_filter]["DSRC_ID"]

    if os.path.splitext(args.output_file_root)[1] == ".json":
        args.output_file_root = os.path.splitext(output_file_root)[0]

    stats_file_name = args.output_file_root + ".json"
    csv_file_name = args.output_file_root + ".csv"
    stat_pack = initialize_stat_pack(stats_file_name, csv_file_name, args.quiet)
    if not stat_pack:
        sys.exit(1)

    entity_sql, min_entity_id, max_entity_id = get_entity_sql(sz_dbo, dsrc_id_filter)
    if not max_entity_id:
        print("No entities found for data source filter!\n")
        sys.exit(1)
    entity_sql = sz_dbo.sqlPrep(entity_sql)

    kwargs = {
        "sz_db_uri": sz_db_uri,
        "api_version": api_version,
        "sz_config_data": sz_config_data,
        "dsrc_id_filter": dsrc_id_filter,
        "sample_size": args.sample_size,
        "relationship_filter": args.relationship_filter,
        "export_csv": args.for_audit,
        "chunk_size": args.chunk_size,
        "thread_count":  args.thread_count,
        "output_file_root": args.output_file_root,
        "stats_file_name": stats_file_name,
        "csv_file_name": csv_file_name,
        "stat_pack": stat_pack
    }

    proc_start_time = time.time()
    queue_processor = IOQueueProcessor(reader, writer, **kwargs)
    print(f"starting {queue_processor.process_count} processes")
    queue_processor.start_up()

    entity_count = 0
    beg_entity_id = min_entity_id
    end_entity_id = beg_entity_id + args.chunk_size
    while True:
        print(f"Getting entities from {beg_entity_id} to {end_entity_id}...")
        batch = sz_dbo.fetchAllRows(sz_dbo.sqlExec(entity_sql, (beg_entity_id, end_entity_id)))
        if batch:
            last_row_entity_id = batch[-1][0]
            for row in batch:
                queue_processor.process(row[0])
                entity_count += 1
                if (entity_count % PROGRESS_INTERVAL == 0 or row[0] == last_row_entity_id):
                    now = datetime.now().strftime("%I:%M%p").lower()
                    elapsed_seconds = round((time.time() - proc_start_time))
                    elapsed_minutes = round(elapsed_seconds/60,1)
                    eps = entity_count/float(elapsed_seconds) if elapsed_seconds else 0
                    print(f"{entity_count} entities processed at {now} after {elapsed_minutes} minutes at {eps} per second")
                if shut_down:
                    break

            queue_processor.wait_for_queues()
            queue_processor.signal_writer( {"STATUS": "Interim", "LAST_ENTITY_ID": end_entity_id})
            queue_processor.wait_for_queues()

        if shut_down:
            break
        if end_entity_id >= max_entity_id:
            break
        
        beg_entity_id += args.chunk_size
        end_entity_id += args.chunk_size

    final_update = {
        "STATUS": "Complete",
        "LAST_ENTITY_ID": end_entity_id,
        "END_TIME": datetime.now().strftime("%m/%d/%Y %H:%M:%S"),
    }
    queue_processor.signal_writer(final_update)
    queue_processor.finish_up()

    elapsed_mins = round((time.time() - proc_start_time) / 60, 1)
    run_status = ('completed in' if not shut_down else 'aborted after') + f' {elapsed_mins:,} minutes'
    if shut_down == 0:
        print(f"Process completed successfully in {elapsed_mins} minutes\n")
    else:
        print(f"Process aborted after {elapsed_mins} minutes!\n")

    sys.exit(shut_down)

