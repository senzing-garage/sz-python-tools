#! /usr/bin/env python3

import argparse
import concurrent.futures
import csv
import itertools
import json
import logging
import multiprocessing
import os
import pathlib
import random
import sys
import textwrap
import time
import traceback
from datetime import datetime
from queue import Empty, Full

try:
    import orjson
except ImportError:
    orjson = False

from senzing_core import SzAbstractFactory, SzEngineFlags, SzError

from _sz_database import SzDatabase
from _tool_helpers import get_engine_config

MODULE_NAME = pathlib.Path(__file__).stem
PROGRESS_INTERVAL = 10000


class IOQueueProcessor:

    def __init__(self, input_class, output_class, **kwargs):

        self.process_count = kwargs.get("process_count", multiprocessing.cpu_count() * 2)
        self.all_stop = multiprocessing.Value("i", 0)

        self.input_class = input_class
        self.output_class = output_class
        self.input_queue = multiprocessing.Queue(self.process_count * 10)
        self.output_queue = multiprocessing.Queue(self.process_count * 10)

        self.kwargs = kwargs
        self.process_list = []

    def start_up(self):
        self.process_list.append(
            multiprocessing.Process(
                target=self.output_queue_reader, args=(0, self.output_queue, self.output_class), kwargs=self.kwargs
            )
        )
        for process_number in range(self.process_count - 1):
            self.process_list.append(
                multiprocessing.Process(
                    target=self.input_queue_reader,
                    args=(process_number + 1, self.input_queue, self.output_queue, self.input_class),
                    kwargs=self.kwargs,
                )
            )
        for process in self.process_list:
            process.start()

    def wait_for_queues(self, max_wait=2):
        waits = 0
        while self.input_queue.qsize() or self.output_queue.qsize():
            size1 = self.input_queue.qsize()
            size2 = self.output_queue.qsize()
            if waits > 1:
                logging.info("waiting for %s input and %s output queue records" % (size1, size2))
            time.sleep(1)
            waits += 1
            if max_wait and waits > max_wait:
                break
        if self.input_queue.qsize() or self.output_queue.qsize():
            logging.warning("queues not empty!")

    def finish_up(self):
        self.wait_for_queues()
        with self.all_stop.get_lock():
            self.all_stop.value = 1

        start = time.time()
        while time.time() - start <= 15:
            if not any(process.is_alive() for process in self.process_list):
                break
            time.sleep(1)

        for process in self.process_list:
            if process.is_alive():
                logging.warning("%s did not terminate gracefully" % process.name)
                process.terminate()
            process.join()

        self.input_queue.close()
        self.output_queue.close()

    def queue_read(self, q):
        try:
            return q.get(True, 1)
        except Empty:
            return None

    def queue_write(self, q, msg):
        while True:
            try:
                q.put(msg, True, 1)
            except Full:
                continue
            break

    def input_queue_reader(self, process_number, input_queue, output_queue, function_ref, **kwargs):
        kwargs["process_number"] = process_number
        input_class = function_ref(**kwargs)

        while self.all_stop.value == 0:
            queue_data = self.queue_read(input_queue)
            if queue_data:
                result = input_class.run(queue_data)
                if result:
                    self.queue_write(output_queue, result)

        input_class.close()

    def output_queue_reader(self, process_number, output_queue, function_ref, **kwargs):

        kwargs["process_number"] = process_number
        output_class = function_ref(**kwargs)

        while self.all_stop.value == 0:
            queue_data = self.queue_read(output_queue)
            if queue_data:
                output_class.run(queue_data)

        output_class.close()

    def process(self, msg):
        self.queue_write(self.input_queue, msg)

    def signal_writer(self, msg):
        self.queue_write(self.output_queue, msg)


class reader:

    def __init__(self, **kwargs):

        self.sz_config_data = kwargs.get("sz_config_data")
        self.relationship_filter = kwargs.get("relationship_filter")

        self.sz_db_uri = kwargs.get("sz_db_uri")
        self.sdk_version = kwargs.get("sdk_version")
        self.sz_config_data = kwargs.get("sz_config_data")
        self.dsrc_lookup = {x["DSRC_ID"]: x for x in sz_config_data["G2_CONFIG"]["CFG_DSRC"]}
        self.dsrc_code_lookup = {x["DSRC_CODE"]: x for x in sz_config_data["G2_CONFIG"]["CFG_DSRC"]}
        self.errule_lookup = {x["ERRULE_ID"]: x for x in sz_config_data["G2_CONFIG"]["CFG_ERRULE"]}
        self.errule_code_lookup = {x["ERRULE_CODE"]: x for x in sz_config_data["G2_CONFIG"]["CFG_ERRULE"]}
        self.errule_code_lookup["DISCLOSED"] = {"ERRULE_ID": 0}
        self.ftype_lookup = {x["FTYPE_ID"]: x for x in sz_config_data["G2_CONFIG"]["CFG_FTYPE"]}
        self.ftype_code_lookup = {x["FTYPE_CODE"]: x for x in sz_config_data["G2_CONFIG"]["CFG_FTYPE"]}
        self.esb_ftype_ids = [x["FTYPE_ID"] for x in sz_config_data["G2_CONFIG"]["CFG_FTYPE"] if x["DERIVED"] == "No"]

        self.sz_dbo = SzDatabase(self.sz_db_uri)
        sql_entities = (
            "select "
            " a.RES_ENT_ID as RESOLVED_ENTITY_ID, "
            " a.ERRULE_ID, "
            " a.MATCH_KEY, "
            " b.DSRC_ID, "
            " c.RECORD_ID "
            "from RES_ENT_OKEY a "
            "join OBS_ENT b on b.OBS_ENT_ID = a.OBS_ENT_ID "
            "join DSRC_RECORD c on c.ENT_SRC_KEY = b.ENT_SRC_KEY and c.DSRC_ID = b.DSRC_ID "
            "where a.RES_ENT_ID = ?"
        )
        self.sql_entities = self.sz_dbo.sqlPrep(sql_entities)
        sql_relations = (
            "select "
            " a.RES_ENT_ID as RESOLVED_ENTITY_ID, "
            " a.REL_ENT_ID as RELATED_ENTITY_ID, "
            " b.LAST_ERRULE_ID as ERRULE_ID, "
            " b.IS_DISCLOSED, "
            " b.IS_AMBIGUOUS, "
            " b.MATCH_KEY, "
            " d.DSRC_ID "
            "from RES_REL_EKEY a "
            "join RES_RELATE b on b.RES_REL_ID = a.RES_REL_ID "
            "join RES_ENT_OKEY c on c.RES_ENT_ID = a.REL_ENT_ID "
            "join OBS_ENT d on d.OBS_ENT_ID = c.OBS_ENT_ID "
            "where a.RES_ENT_ID = ?"
        )
        self.sql_relations = self.sz_dbo.sqlPrep(sql_relations)
        # below not currently used in favor of sdk as does a better job identifying unique features
        sql_features = (
            "select "
            " FTYPE_ID "
            "from RES_FEAT_EKEY a "
            "where RES_ENT_ID = ?"
            "and FTYPE_ID in (" + ",".join(str(x) for x in self.esb_ftype_ids) + ")"
            "and SUPPRESSED = 'N'"
        )
        self.sql_features = self.sz_dbo.sqlPrep(sql_features)

    def close(self):
        self.sz_dbo.close()

    def run(self, queue_data):
        if queue_data[0] == "RESUME":
            entity_id = queue_data[1]
            resume_rows = []
            cursor = self.sz_dbo.sqlExec(self.sql_entities, [entity_id])
            for row_data in self.sz_dbo.fetchAllDicts(cursor):
                row_data = self.complete_resume_db(row_data)
                resume_rows.append(row_data)
            if resume_rows and self.relationship_filter in (2, 3):
                cursor = self.sz_dbo.sqlExec(self.sql_relations, [entity_id])
                for row_data in self.sz_dbo.fetchAllDicts(cursor):
                    row_data = self.complete_resume_db(row_data)
                    resume_rows.append(row_data)
            return (queue_data[0], resume_rows)

        elif queue_data[0] == "REVIEW":
            # not currently used in favor of sdk as does a better job identifying unique features
            entity_size, entity_id, review_features = self.review_features(queue_data)
            return ("REVIEW", entity_size, entity_id, review_features)
        return None

    def complete_resume_db(self, row_data):
        if "RELATED_ENTITY_ID" not in row_data:
            row_data["RELATED_ENTITY_ID"] = 0
            row_data["IS_DISCLOSED"] = 0
            row_data["IS_AMBIGUOUS"] = 0
        if "RECORD_ID" not in row_data:
            row_data["RECORD_ID"] = "n/a"
        row_data["DATA_SOURCE"] = self.dsrc_lookup[row_data["DSRC_ID"]]["DSRC_CODE"]
        if not row_data["ERRULE_ID"]:
            row_data["ERRULE_CODE"] = "unk"
        else:
            row_data["ERRULE_CODE"] = self.errule_lookup[row_data["ERRULE_ID"]]["ERRULE_CODE"]

        if row_data["RELATED_ENTITY_ID"] == 0:
            row_data["MATCH_LEVEL"] = 1 if row_data["ERRULE_CODE"] else 0
        elif row_data["IS_DISCLOSED"] != 0:
            row_data["MATCH_LEVEL"] = 11
        else:
            row_data["MATCH_LEVEL"] = self.errule_lookup[row_data["ERRULE_ID"]]["RTYPE_ID"]
        return row_data

    def review_features(self, queue_data):
        entity_id = queue_data[1]
        entity_size = queue_data[2]
        cursor = self.sz_dbo.sqlExec(self.sql_features, [entity_id])
        features = {}
        if entity_size > 1:
            for row in self.sz_dbo.fetchAllRows(cursor):
                ftype_code = self.ftype_lookup[row[0]]["FTYPE_CODE"]
                if ftype_code not in features:
                    features[ftype_code] = 1
                else:
                    features[ftype_code] += 1

        return entity_size, entity_id, features


class writer:

    def __init__(self, **kwargs):
        self.stat_pack = kwargs.get("stat_pack")
        self.sample_size = kwargs.get("sample_size")
        self.export_csv = kwargs.get("for_audit")
        self.stats_file_name = kwargs.get("stats_file_name")
        self.csv_file_name = kwargs.get("csv_file_name")
        self.match_levels = ["MATCH", "AMBIGUOUS_MATCH", "POSSIBLE_MATCH", "POSSIBLE_RELATION", "DISCLOSED_RELATION"]
        if not self.stat_pack:
            self.initialize_stat_pack()
        self.esb_features = {}

        if self.export_csv:
            column_headers = [
                "RESOLVED_ENTITY_ID",
                "RELATED_ENTITY_ID",
                "MATCH_LEVEL",
                "MATCH_KEY",
                "DATA_SOURCE",
                "RECORD_ID",
            ]
            self.csv_handle = open(self.csv_file_name, "a", encoding="utf-8")
            self.csv_writer = csv.writer(self.csv_handle)
            self.csv_writer.writerow(column_headers)

    def close(self):
        return None

    def run(self, queue_data):
        if queue_data[0] == "RESUME":
            self.compute_stats(queue_data[1])
            if self.export_csv:
                csv_rows = [
                    [
                        x["RESOLVED_ENTITY_ID"],
                        x["RELATED_ENTITY_ID"],
                        x["MATCH_LEVEL"],
                        x["MATCH_KEY"],
                        x["DATA_SOURCE"],
                        x["RECORD_ID"],
                    ]
                    for x in queue_data[1]
                ]
                self.csv_writer.writerows(csv_rows)

        elif queue_data[0] == "REVIEW":  # not currently used as sdk used to gather stats
            entity_size = queue_data[1]
            sample = {queue_data[2]: queue_data[3]}
            if entity_size not in self.esb_features:
                self.esb_features[entity_size] = {"COUNT": 1, "SAMPLE": [sample]}
            else:
                self.esb_features[entity_size]["COUNT"] += 1
                self.esb_features[entity_size]["SAMPLE"].append(sample)

        elif queue_data[0] == "DUMP_STATS":
            self.stat_pack["PROCESS"].update(queue_data[1])
            if self.stat_pack["PROCESS"]["STATUS"] == "Complete":
                if self.esb_features:
                    self.stat_pack["ENTITY_SIZES"] = self.esb_features
                if self.export_csv:
                    self.csv_handle.close()
            with open(self.stats_file_name, "w", encoding="utf-8") as outfile:
                json.dump(self.stat_pack, outfile, indent=4)

    def get_random_index(self):
        target_index = int(self.sample_size * random.random())
        if target_index % 10 != 0:
            return target_index
        return 0

    def update_stat_pack(self, prior_keys, stats):
        d = self.stat_pack
        for key in prior_keys:
            if not d.get(key):
                d[key] = {}
            d = d[key]
        for k, v in stats.items():
            if k not in d:
                d[k] = v
            else:
                d[k] += v

    def initialize_stat_pack(self):
        self.stat_pack = {
            "SOURCE": "sz_snapshot",
            "PROCESS": {
                "STATUS": "Incomplete",
                "START_TIME": datetime.now().strftime("%m/%d/%Y %H:%M:%S"),
                "LAST_ENTITY_ID": 0,
            },
            "TOTALS": {"ENTITY_COUNT": 0, "RECORD_COUNT": 0},
            "DATA_SOURCES": {},
            "CROSS_SOURCES": {},
            "MULTI_SOURCES": {},
            "ENTITY_SIZES": {},
        }
        self.initialize_match_levels(["TOTALS"])

    def initialize_match_levels(self, stat_keys):
        for _match_level in self.match_levels:
            if _match_level == "MATCH":
                stats = {"ENTITY_COUNT": 0, "RECORD_COUNT": 0}
            else:
                stats = {"RELATION_COUNT": 0}
            self.update_stat_pack(stat_keys + [_match_level], stats)

    def compute_stats(self, resume_rows):
        record_list = []
        resume_data = {}
        entity_size = 0
        entity_id = resume_rows[0]["RESOLVED_ENTITY_ID"]
        self.random_index = self.get_random_index() if entity_id % 10 == 0 else 0
        for row_data in resume_rows:
            related_id = row_data["RELATED_ENTITY_ID"]
            data_source = row_data["DATA_SOURCE"]
            record_id = row_data["RECORD_ID"]
            principle = f"{row_data['ERRULE_ID']}: {row_data['ERRULE_CODE']}" if row_data["ERRULE_ID"] else ""
            match_key = row_data["MATCH_KEY"]
            if related_id == 0:
                match_level = "MATCH"
                entity_size += 1
                record_list.append(data_source + ":" + record_id)
            elif row_data["IS_DISCLOSED"] != 0:
                match_level = "DISCLOSED_RELATION"
                principle = "DISCLOSURE"
            elif row_data["IS_AMBIGUOUS"] != 0:
                match_level = "AMBIGUOUS_MATCH"
            elif row_data["MATCH_LEVEL"] == 2:
                match_level = "POSSIBLE_MATCH"
            else:
                match_level = "POSSIBLE_RELATION"

            if related_id not in resume_data:
                resume_data[related_id] = {}
                resume_data[related_id]["MATCH_LEVEL"] = match_level
                resume_data[related_id]["DATA_SOURCES"] = {}
                resume_data[related_id]["PRINCIPLES"] = {}

            if data_source not in resume_data[related_id]["DATA_SOURCES"]:
                resume_data[related_id]["DATA_SOURCES"][data_source] = {"COUNT": 1, "PRINCIPLES": []}
            else:
                resume_data[related_id]["DATA_SOURCES"][data_source]["COUNT"] += 1

            if principle:
                principle_matchkey = f"{principle}||{match_key}"
                if principle_matchkey not in resume_data[related_id]["DATA_SOURCES"][data_source]["PRINCIPLES"]:
                    resume_data[related_id]["DATA_SOURCES"][data_source]["PRINCIPLES"].append(principle_matchkey)
                if principle_matchkey not in resume_data[related_id]["PRINCIPLES"]:
                    resume_data[related_id]["PRINCIPLES"][principle_matchkey] = 1
                else:
                    resume_data[related_id]["PRINCIPLES"][principle_matchkey] += 1

        self.update_stat_pack(["TOTALS"], {"ENTITY_COUNT": 1})
        self.update_stat_pack(["TOTALS"], {"RECORD_COUNT": entity_size})
        self.update_stat_pack(["ENTITY_SIZES", entity_size], {"COUNT": 1, "SAMPLE": [entity_id]})

        entity0_sources = []  # should get set by entity 0 as list is sorted
        for related_id in sorted(resume_data.keys()):
            match_level = resume_data[related_id]["MATCH_LEVEL"]
            if related_id == 0:
                for principle_matchkey in resume_data[related_id]["PRINCIPLES"]:
                    record_count = resume_data[related_id]["PRINCIPLES"][principle_matchkey]
                    stat_keys = ["TOTALS", match_level]
                    self.update_stat_pack(stat_keys, {"ENTITY_COUNT": 1, "RECORD_COUNT": record_count})
                    stat_keys = ["TOTALS", match_level, "PRINCIPLES", principle_matchkey]
                    self.update_stat_pack(stat_keys, {"COUNT": 1, "SAMPLE": [entity_id]})
                entity0_sources = resume_data[related_id]["DATA_SOURCES"]
                for data_source1 in entity0_sources:
                    if data_source1 not in self.stat_pack["DATA_SOURCES"]:
                        self.initialize_match_levels(["DATA_SOURCES", data_source1])
                    record_cnt = entity0_sources[data_source1]["COUNT"]
                    for data_source2 in entity0_sources:
                        if data_source2 == data_source1:
                            stat_keys = ["DATA_SOURCES", data_source1]
                            self.update_stat_pack(stat_keys, {"ENTITY_COUNT": 1, "RECORD_COUNT": record_cnt})
                            if record_cnt > 1:
                                stat_keys.append(match_level)
                                self.update_stat_pack(stat_keys, {"ENTITY_COUNT": 1, "RECORD_COUNT": record_cnt})
                                if len(entity0_sources[data_source1]["PRINCIPLES"]) == 1:
                                    principle_matchkey = entity0_sources[data_source1]["PRINCIPLES"][0]
                                elif len(entity0_sources[data_source1]["PRINCIPLES"]) > 1:
                                    principle_matchkey = "multiple||multiple"
                                else:
                                    principle_matchkey = "none||none"
                                stat_keys.extend(["PRINCIPLES", principle_matchkey])
                                self.update_stat_pack(stat_keys, {"COUNT": 1, "SAMPLE": [entity_id]})
                        else:
                            data_source_pair = f"{data_source1}||{data_source2}"
                            if data_source_pair not in self.stat_pack["CROSS_SOURCES"]:
                                self.initialize_match_levels(["CROSS_SOURCES", data_source_pair])
                            stat_keys = ["CROSS_SOURCES", data_source_pair, match_level]
                            self.update_stat_pack(stat_keys, {"ENTITY_COUNT": 1, "RECORD_COUNT": record_cnt})
                            if len(entity0_sources[data_source2]["PRINCIPLES"]) == 1:
                                principle_matchkey = entity0_sources[data_source2]["PRINCIPLES"][0]
                            elif len(entity0_sources[data_source2]["PRINCIPLES"]) > 1:
                                principle_matchkey = "multiple||multiple"
                            elif len(entity0_sources[data_source1]["PRINCIPLES"]) == 1:
                                principle_matchkey = entity0_sources[data_source1]["PRINCIPLES"][0]
                            else:
                                principle_matchkey = "indeterminate||indeterminate"
                            stat_keys.extend(["PRINCIPLES", principle_matchkey])
                            self.update_stat_pack(stat_keys, {"COUNT": 1, "SAMPLE": [entity_id]})

                # if len(entity0_sources) > 1:
                # include single source so can find non-matches, ie customers not on watch list or in reference file
                multi_source_key = "||".join(sorted(entity0_sources.keys()))
                self.update_stat_pack(["MULTI_SOURCES", multi_source_key], {"ENTITY_COUNT": 1, "SAMPLE": [entity_id]})
            elif related_id > entity_id:
                sample = f"{entity_id} {related_id}"
                principle_matchkey = list(resume_data[related_id]["PRINCIPLES"].keys())[0]
                record_count = resume_data[related_id]["PRINCIPLES"][principle_matchkey]
                stat_keys = ["TOTALS", match_level]
                self.update_stat_pack(stat_keys, {"RELATION_COUNT": 1})
                stat_keys = ["TOTALS", match_level, "PRINCIPLES", principle_matchkey]
                self.update_stat_pack(stat_keys, {"COUNT": 1, "SAMPLE": [sample]})

                for data_source1 in entity0_sources:
                    for data_source2 in resume_data[related_id]["DATA_SOURCES"]:
                        if data_source1 == data_source2:
                            stat_keys = ["DATA_SOURCES", data_source1, match_level]
                        else:
                            data_source_pair = f"{data_source1}||{data_source2}"
                            if data_source_pair not in self.stat_pack["CROSS_SOURCES"]:
                                self.initialize_match_levels(["CROSS_SOURCES", data_source_pair])
                            stat_keys = ["CROSS_SOURCES", data_source_pair, match_level]
                        self.update_stat_pack(stat_keys, {"RELATION_COUNT": 1})
                        stat_keys.extend(["PRINCIPLES", principle_matchkey])
                        self.update_stat_pack(stat_keys, {"COUNT": 1, "SAMPLE": [sample]})


def check_stat_pack(stats_file_name, csv_file_name, args):
    abort = False
    stat_pack = json.load(open(stats_file_name))
    prior_status = "unknown"
    entity_count = 0
    if stat_pack.get("PROCESS"):
        prior_status = stat_pack["PROCESS"]["STATUS"]
        entity_count = stat_pack["TOTALS"]["ENTITY_COUNT"]
    print(f"\n{prior_status} snapshot file exists with {entity_count} entities processed!")
    if args.quiet:
        print("PRIOR FILES WILL BE OVERWRITTEN")
        stat_pack = {}
    else:
        if prior_status == "Interim":
            if args.force_sdk:
                ans = input("\nDo you want to overwrite it? (y/n) ")
                if ans.upper().startswith("Y"):
                    stat_pack = {}
                else:
                    abort = True
            else:
                ans = input("\nDo you want to pick up where it left off? (y/n) ")
                if not ans.upper().startswith("Y"):
                    stat_pack = {}
        elif prior_status == "Complete":
            ans = input("\nAre you sure you want to overwrite it? (y/n) ")
            if ans.upper().startswith("Y"):
                stat_pack = {}
            else:
                abort = True
        else:
            stat_pack = {}
        print()
    if not stat_pack and os.path.exists(csv_file_name):
        os.remove(csv_file_name)
    return stat_pack, abort


def progress_display(started, cntr):
    elapsed_seconds = round((time.time() - started))
    elapsed_minutes = round(elapsed_seconds / 60, 1)
    eps = cntr / float(elapsed_seconds) if elapsed_seconds else 0
    parms = (cntr, elapsed_minutes, eps)
    logging.info("%s entities processed after %s minutes at %s per second" % parms)


def database_snapshot(sz_dbo, kwargs):
    logging.info("Determining entity range ...")
    proc_start_time = kwargs.get("proc_start_time")
    dsrc_id_filter = kwargs.get("dsrc_id_filter")
    stat_pack = kwargs.get("stat_pack")
    chunk_size = kwargs.get("chunk_size")

    if not dsrc_id_filter:
        max_sql = "select max(RES_ENT_ID) from RES_ENT"
        min_entity_id = 0
        max_entity_id = sz_dbo.fetchRow(sz_dbo.sqlExec(max_sql))[0]
        entity_sql = "select RES_ENT_ID from RES_ENT where RES_ENT_ID between ? and ?"
    else:
        max_sql = (
            "select  "
            " min(b.RES_ENT_ID), "
            " max(b.RES_ENT_ID) "
            "from OBS_ENT a "
            "join RES_ENT_OKEY b on b.OBS_ENT_ID = a.OBS_ENT_ID "
            "where a.DSRC_ID = " + dsrc_id_filter
        )
        min_entity_id, max_entity_id = sz_dbo.fetchRow(sz_dbo.sqlExec(max_sql))
        entity_sql = (
            "select distinct"
            " a.RES_ENT_ID "
            "from RES_ENT_OKEY a "
            "join OBS_ENT b on b.OBS_ENT_ID = a.OBS_ENT_ID "
            "where a.RES_ENT_ID between ? and ? and b.DSRC_ID = " + str(dsrc_id_filter)
        )

    if not max_entity_id:
        logging.error("No entities found for data source filter!")
        sys.exit(1)
    entity_sql = sz_dbo.sqlPrep(entity_sql)

    queue_processor = IOQueueProcessor(reader, writer, **kwargs)
    logging.info("starting %s processes" % queue_processor.process_count)
    queue_processor.start_up()

    entity_count = 0
    beg_entity_id = stat_pack.get("PROCESS", {}).get("LAST_ENTITY_ID", min_entity_id)
    end_entity_id = beg_entity_id + chunk_size
    while True:
        logging.info("Getting entities from %s to %s ..." % (beg_entity_id, end_entity_id))
        batch = sz_dbo.fetchAllRows(sz_dbo.sqlExec(entity_sql, (beg_entity_id, end_entity_id)))
        if batch:
            last_row_entity_id = batch[-1][0]
            for row in batch:
                queue_processor.process(("RESUME", row[0]))
                entity_count += 1
                if entity_count % PROGRESS_INTERVAL == 0 or row[0] == last_row_entity_id:
                    progress_display(proc_start_time, entity_count)

            queue_processor.wait_for_queues()
            queue_processor.signal_writer(("DUMP_STATS", {"STATUS": "Interim", "LAST_ENTITY_ID": end_entity_id}))

        if end_entity_id >= max_entity_id:
            break
        beg_entity_id += chunk_size
        end_entity_id += chunk_size
    queue_processor.finish_up()


def next_sdk_record(sz_engine, export_handle, errule_code_lookup, export_headers=None):
    row_string = sz_engine.fetch_next(export_handle)
    if not row_string:
        return None
    csv_row = next(csv.reader([row_string]))
    if not export_headers:
        return csv_row
    row_data = dict(zip(export_headers, csv_row))
    row_data["RESOLVED_ENTITY_ID"] = int(row_data["RESOLVED_ENTITY_ID"])
    row_data["RELATED_ENTITY_ID"] = int(row_data["RELATED_ENTITY_ID"])
    row_data["IS_DISCLOSED"] = int(row_data["IS_DISCLOSED"])
    row_data["IS_AMBIGUOUS"] = int(row_data["IS_AMBIGUOUS"])
    row_data["MATCH_LEVEL"] = int(row_data["MATCH_LEVEL"])
    if row_data["IS_DISCLOSED"] != 0:
        row_data["MATCH_LEVEL"] = 11
    if row_data["ERRULE_CODE"]:
        row_data["ERRULE_ID"] = errule_code_lookup[row_data["ERRULE_CODE"]]["ERRULE_ID"]
    else:
        row_data["ERRULE_ID"] = 0
    return row_data


def sdk_snapshot(sz_engine, kwargs):
    logging.info("Starting sdk export ...")
    proc_start_time = kwargs.get("proc_start_time")
    stat_writer = writer(**kwargs)
    sz_config_data = kwargs.get("sz_config_data")
    relationship_filter = kwargs.get("relationship_filter")
    errule_code_lookup = {x["ERRULE_CODE"]: x for x in sz_config_data["G2_CONFIG"]["CFG_ERRULE"]}
    errule_code_lookup["DISCLOSED"] = {"ERRULE_ID": 0}
    export_flags = SzEngineFlags.SZ_EXPORT_INCLUDE_ALL_ENTITIES
    if relationship_filter == 1:
        pass  # --don't include any relationships
    elif relationship_filter == 2:
        export_flags = export_flags | SzEngineFlags.SZ_ENTITY_INCLUDE_POSSIBLY_SAME_RELATIONS
    else:
        export_flags = export_flags | SzEngineFlags.SZ_ENTITY_INCLUDE_ALL_RELATIONS
    export_fields = [
        "RESOLVED_ENTITY_ID",
        "RELATED_ENTITY_ID",
        "MATCH_LEVEL",
        "MATCH_KEY",
        "IS_DISCLOSED",
        "IS_AMBIGUOUS",
        "ERRULE_CODE",
        "DATA_SOURCE",
        "RECORD_ID",
    ]
    try:
        export_handle = sz_engine.export_csv_entity_report(",".join(export_fields), export_flags)
        export_headers = next_sdk_record(sz_engine, export_handle, None)
    except SzError as err:
        raise err

    last_entity_id = -1
    entity_count = 0
    record = next_sdk_record(sz_engine, export_handle, errule_code_lookup, export_headers)
    while record:
        resume_rows = []
        last_entity_id = record["RESOLVED_ENTITY_ID"]
        while record and record["RESOLVED_ENTITY_ID"] == last_entity_id:
            resume_rows.append(record)
            record = next_sdk_record(sz_engine, export_handle, errule_code_lookup, export_headers)
        stat_writer.run(("RESUME", resume_rows))

        entity_count += 1
        if entity_count % PROGRESS_INTERVAL == 0:
            progress_display(proc_start_time, entity_count)

    progress_display(proc_start_time, entity_count)
    stat_writer.run(("DUMP_STATS", {"STATUS": "Interim", "LAST_ENTITY_ID": last_entity_id}))


def lookup_esb_features(sz_engine, record):
    entity_size, entity_id = record
    try:
        response = sz_engine.get_entity_by_entity_id(
            int(entity_id), SzEngineFlags.SZ_ENTITY_INCLUDE_REPRESENTATIVE_FEATURES
        )
    except SzError as err:
        logging.warning(err)
        return entity_size, entity_id, {}
    json_data = orjson.loads(response) if orjson else json.loads(response)
    features = {}
    for ftype_code in json_data["RESOLVED_ENTITY"]["FEATURES"]:
        features[ftype_code] = len(json_data["RESOLVED_ENTITY"]["FEATURES"][ftype_code])
    return entity_size, entity_id, features


def compute_esb_stats(stat_pack, sz_engine):

    entity_list = []
    entity_sizes = {}
    for entity_size in stat_pack["ENTITY_SIZES"]:
        count = stat_pack["ENTITY_SIZES"][entity_size]["COUNT"]
        if entity_size not in entity_sizes:
            entity_sizes[entity_size] = {"COUNT": count, "SAMPLE": []}
        for entity_id in stat_pack["ENTITY_SIZES"][entity_size]["SAMPLE"]:
            entity_list.append([entity_size, entity_id])

    logging.info("Reviewing %s entities" % len(entity_list))
    entity_list_iterator = iter(entity_list)
    cnt = 0
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(lookup_esb_features, sz_engine, record): record
            for record in itertools.islice(entity_list_iterator, executor._max_workers)
        }
        while futures:
            done, _ = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)
            for f in done:
                entity_size, entity_id, features = f.result()
                entity_sizes[entity_size]["SAMPLE"].append({entity_id: features})
                del futures[f]
                cnt += 1
                if cnt % 1000 == 0:
                    logging.info("%s entities processed" % cnt)
                record = next(entity_list_iterator, None)
                if record:
                    futures[executor.submit(lookup_esb_features, sz_engine, record)] = record
    logging.info("%s entities processed, complete!" % cnt)
    return entity_sizes


def debug_print(_value, _desc="some variable"):
    print("-" * 20)
    print(_desc)
    if type(_value) in (list, dict):
        print(json.dumps(_value, indent=4))
    else:
        print(_value)
    input("press any key ...")


def print_exception_info():
    print(traceback.format_exc())


if __name__ == "__main__":

    output_file_root = os.getenv("SENZING_OUTPUT_FILE_ROOT", None)
    env_size = os.getenv("SENZING_SAMPLE_SIZE", "")
    sample_size = int(env_size) if env_size and env_size.isdigit() else 1000
    datasource_filter = os.getenv("SENZING_DATASOURCE_FILTER", None)
    env_filter = os.getenv("SENZING_RELATIONSHIP_FILTER", None)
    relationship_filter = int(env_filter) if env_filter and env_filter.isdigit() else 3
    env_chunk = os.getenv("SENZING_CHUNK_SIZE", None)
    chunk_size = int(env_chunk) if env_chunk and env_chunk.isdigit() else 1000000
    env_thread = os.getenv("SENZING_THREAD_COUNT", None)
    thread_count = int(env_thread) if env_thread and env_thread.isdigit() else 0

    parser = argparse.ArgumentParser()
    parser.add_argument("-o", "--output_file_root", default=output_file_root, help="root name for files to be created")
    parser.add_argument("-c", "--config_file_name", help="Path and name of optional G2Module.ini file to use.")
    parser.add_argument("-s", "--sample_size", type=int, default=sample_size, help="the number of samples to log")
    parser.add_argument("-d", "--dsrc_filter", help="optional data source code to analayze")
    help = "filter options 1=No Relationships, 2=Include possible matches, 3=Include possibly related and disclosed"
    parser.add_argument("-f", "--relationship_filter", type=int, default=relationship_filter, help=help)
    parser.add_argument("-k", "--chunk_size", type=int, default=chunk_size, help="records per batch")
    parser.add_argument("-t", "--thread_count", type=int, default=thread_count, help="number of threads to start")
    parser.add_argument("-F", "--force_sdk", action="store_true", default=False, help="force sdk export")
    parser.add_argument("-A", "--for_audit", action="store_true", default=False, help="export csv file for audit")
    parser.add_argument("-Q", "--quiet", action="store_true", default=False, help="overwrite withpout warning")
    parser.add_argument("-D", "--debug", dest="debug", action="store_true", default=False, help="run in debug mode")
    args = parser.parse_args()
    loggingLevel = logging.DEBUG if args.debug else logging.INFO
    logging.basicConfig(format="%(asctime)s %(levelname)s: %(message)s", datefmt="%m/%d %I:%M", level=loggingLevel)

    if not args.output_file_root:
        print("Please use -o to select an output path and root file name such as /project/audit/snap1")
        sys.exit(1)

    try:
        engine_config = get_engine_config(args.config_file_name)
        sz_factory = SzAbstractFactory(MODULE_NAME, engine_config)
        sz_configmgr = sz_factory.create_configmanager()
        sz_engine = sz_factory.create_engine()
        sz_product = sz_factory.create_product()
        sdk_version = json.loads(sz_product.get_version())
        sz_config_data = json.loads(sz_configmgr.get_config(sz_configmgr.get_default_config_id()))
        sz_db_uri = json.loads(engine_config)["SQL"]["CONNECTION"]
    except SzError:
        print_exception_info
        sys.exit(1)

    try:
        sz_dbo = SzDatabase(sz_db_uri)
    except Exception as err:
        print(f"\n{err}")
        print(
            textwrap.dedent(
                """\
                    
            Direct database access not available, performing an SDK only snapshot at this time. Installing direct database access
            drivers can significantly reduce the time it takes to snapshot on very large databases.

            """
            )
        )

        if not args.quiet:
            if input("Continue? (y/n) ").lower() not in ("y", "yes"):
                sys.exit(1)
        args.force_sdk = True

    dsrc_id_filter = None
    if args.dsrc_filter:
        dsrc_code_lookup = {item["DSRC_CODE"]: item for item in sz_config_data["G2_CONFIG"]["CFG_DSRC"]}
        if not dsrc_code_lookup.get(args.dsrc_filter):
            logging.error("Data source code %s is not valid", args.dsrc_filter)
            sys.exit(1)
        else:
            dsrc_id_filter = dsrc_code_lookup[args.dsrc_filter]["DSRC_ID"]

    if os.path.splitext(args.output_file_root)[1] == ".json":
        args.output_file_root = os.path.splitext(output_file_root)[0]

    stats_file_name = args.output_file_root + ".json"
    csv_file_name = args.output_file_root + ".csv"
    stat_pack = {}
    if os.path.exists(stats_file_name):
        stat_pack, abort = check_stat_pack(stats_file_name, csv_file_name, args)
        if abort:
            sys.exit(1)

    proc_start_time = time.time()
    kwargs = {
        "sz_db_uri": sz_db_uri,
        "sdk_version": sdk_version,
        "sz_config_data": sz_config_data,
        "dsrc_id_filter": dsrc_id_filter,
        "sample_size": args.sample_size,
        "relationship_filter": args.relationship_filter,
        "for_audit": args.for_audit,
        "chunk_size": args.chunk_size,
        "thread_count": args.thread_count,
        "output_file_root": args.output_file_root,
        "stats_file_name": stats_file_name,
        "csv_file_name": csv_file_name,
        "stat_pack": stat_pack,
        "proc_start_time": proc_start_time,
    }
    if args.force_sdk:
        sdk_snapshot(sz_engine, kwargs)
    else:
        database_snapshot(sz_dbo, kwargs)

    with open(stats_file_name, "r") as f:
        stat_pack = json.load(f)
        stat_pack["ENTITY_SIZES"] = compute_esb_stats(stat_pack, sz_engine)
        stat_pack["PROCESS"]["STATUS"] = "Complete"
        stat_pack["PROCESS"]["END_DATE"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    with open(stats_file_name, "w") as f:
        json.dump(stat_pack, f, indent=4)

    elapsed_mins = round((time.time() - proc_start_time) / 60, 1)
    logging.info(f"Process completed successfully in {elapsed_mins} minutes")
    print()
    sys.exit(0)
